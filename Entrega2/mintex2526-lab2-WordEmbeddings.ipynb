{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c792833e",
   "metadata": {},
   "source": [
    "**Master Universitario en Bioinformática y Biología Computacional, UAM**\n",
    "## **Minería de texto 2025-26**\n",
    "# **Práctica de laboratorio 2: Word embeddings**\n",
    "\n",
    "---\n",
    "\n",
    "_Aviso_: algunas de las explicaciones y ejemplos de este notebook proceden de [Documentación de GenSim en NLTK](https://www.nltk.org/howto/gensim.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66eab8",
   "metadata": {},
   "source": [
    "Antes de comenzar a trabajar, debemos configurar el entorno: \n",
    "- NLTK\n",
    "- GenSim: permite trabajar con embeddings de palabras empleando Word2Vec, FastText y Doc2Vec.\n",
    "\n",
    "Si no lo tenemos ya instalado, instalemos NLTK:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02062e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1bc23a",
   "metadata": {},
   "source": [
    "Ahora instalemos GenSim para poder trabajar con Word2Vec:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9c6257",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f19325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\daniel parra\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\daniel parra\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.15.2)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.4.4-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart_open>=1.8.1->gensim)\n",
      "  Downloading wrapt-2.0.0-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Downloading gensim-4.4.0-cp312-cp312-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 6.0/24.4 MB 30.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 13.1/24.4 MB 31.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.7/24.4 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 32.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 25.7 MB/s eta 0:00:00\n",
      "Downloading smart_open-7.4.4-py3-none-any.whl (63 kB)\n",
      "Downloading wrapt-2.0.0-cp312-cp312-win_amd64.whl (60 kB)\n",
      "Installing collected packages: wrapt, smart_open, gensim\n",
      "Successfully installed gensim-4.4.0 smart_open-7.4.4 wrapt-2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Si necesario (yo sí necesitaba instalar ambos)\n",
    "\n",
    "#!pip install pytest\n",
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e086f7d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.test.gensim_fixt import setup_module\n",
    "setup_module() # Puede que antes tengamos que ejecutar: !pip install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ada554",
   "metadata": {},
   "source": [
    "Si no está ya instalado, descarguemos el corpus Brown de entre los disponibles en NLTK. Este es un corpus en inglés creado en 1961 en la Universidad de Brown. Cuenta con un millón de palabras con texto de 500 fuentes (noticias, editorial, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57be1dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\Daniel\n",
      "[nltk_data]     Parra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49c84f",
   "metadata": {},
   "source": [
    "Para probar, nos bastará con tomar 10000 oraciones del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09069efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "train_set = brown.sents()[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb91285f",
   "metadata": {},
   "source": [
    "Entrenamos el modelo empleando el Word2Vec de GenSim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ef3a5be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fad45e",
   "metadata": {},
   "source": [
    "Si se usa un corpus muy grande, puede llevar mucho tiempo entrenarlo. Lo más cómodo en esos casos es guardarlo y volver a cargarlo:\n",
    "\n",
    "- `model.save('brown.embedding')` # para guardarlo\n",
    "\n",
    "- `model = gensim.models.Word2Vec.load('brown.embedding')` # para recuperarlo\n",
    "\n",
    "Veamos cuántas palabras contiene el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21bbe75a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo contiene 219770 palabras.\n"
     ]
    }
   ],
   "source": [
    "tot_palabras = model.corpus_total_words\n",
    "\n",
    "print('El modelo contiene', tot_palabras, 'palabras.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21425274",
   "metadata": {},
   "source": [
    "¿Cuáles son las 100 palabras más frecuentes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b70914f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las 100 palabras más frecuentes son: ['the', ',', '.', 'of', 'and', 'to', 'a', 'in', 'is', 'that', 'for', '``', 'The', \"''\", 'was', 'as', 'on', 'with', 'be', 'it', 'by', 'at', 'he', 'his', 'are', ';', 'not', 'has', 'from', 'have', 'this', 'will', 'an', '--', 'which', 'who', '?', 'or', 'but', 'had', 'would', 'I', 'one', 'said', 'were', 'they', 'all', 'been', 'their', 'more', 'its', ':', ')', '(', 'He', 'Mr.', 'we', 'than', 'out', 'It', 'In', 'up', 'But', 'there', 'other', 'when', 'can', 'no', 'about', 'only', 'first', 'new', 'him', 'A', 'into', 'so', 'last', 'Mrs.', 'two', 'some', 'them', 'what', 'time', 'her', 'do', 'any', 'our', 'man', 'most', 'also', 'years', 'could', 'over', 'year', 'may', 'New', 'made', 'if', 'you', 'American']\n"
     ]
    }
   ],
   "source": [
    "top_freq_100 = model.wv.index_to_key[:100]\n",
    "\n",
    "print('Las 100 palabras más frecuentes son:', top_freq_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42919f3",
   "metadata": {},
   "source": [
    "Una vez entrenado, el modelo contendrá la lista de palabras en el vocabulario del corpus procesado, junto con el embedding correspondiente para cada palabra.\n",
    "\n",
    "Puede obtenese el embedding de una palabra accediendo al diccionario `wv` (word vector) que tiene el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eff8922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03749723, -0.0295914 ,  0.04599127, -0.12271926, -0.01197173,\n",
       "       -0.37937376,  0.12915371,  0.3983164 , -0.00201501, -0.29717222,\n",
       "       -0.05841655, -0.29605252, -0.15677501,  0.2553639 ,  0.01256201,\n",
       "       -0.0603805 ,  0.05082991, -0.19894981,  0.04807915, -0.3042397 ,\n",
       "        0.0881337 ,  0.1571893 ,  0.37233946, -0.20820516,  0.02570407,\n",
       "       -0.04859156, -0.14490137,  0.0042571 , -0.15714853, -0.00573632,\n",
       "        0.12968998, -0.07610706,  0.08253661, -0.20087884, -0.12298757,\n",
       "        0.12013808,  0.08241677, -0.16388424, -0.24467468, -0.14254531,\n",
       "        0.10262974, -0.14742975, -0.20263787,  0.01358297,  0.07975667,\n",
       "       -0.11571495, -0.25371632, -0.02528531,  0.15248752,  0.08875555,\n",
       "       -0.00689642, -0.20291635,  0.06701262,  0.00181449,  0.00870941,\n",
       "        0.12618214,  0.18857834, -0.11614159, -0.16957398,  0.03920883,\n",
       "        0.00408351,  0.12499843,  0.10471388,  0.07070123, -0.14658125,\n",
       "        0.31985494, -0.08597566,  0.22293088, -0.14872533,  0.03547784,\n",
       "        0.08780412,  0.27284607,  0.21221556,  0.14947751,  0.20048799,\n",
       "       -0.0263638 ,  0.09388658, -0.00420207, -0.05527822,  0.08299303,\n",
       "       -0.2117349 ,  0.11235161, -0.03308514,  0.32294416, -0.06855639,\n",
       "       -0.06162364,  0.15995784,  0.18593706,  0.06924884,  0.16413383,\n",
       "        0.18598959,  0.03559696, -0.08789423,  0.01301243,  0.26737806,\n",
       "        0.2090985 ,  0.00745492, -0.3110798 , -0.04058922, -0.03009149],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['university']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a8fc0",
   "metadata": {},
   "source": [
    "y para calcular la similitud empleando la distancia coseno entre palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78d2b9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9938205"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('university','school')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e58cc7",
   "metadata": {},
   "source": [
    "Usando un modelo preentrenado. Podemos descargarlo, o emplear uno de los que tiene nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fb69868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to C:\\Users\\Daniel\n",
      "[nltk_data]     Parra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping models\\word2vec_sample.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('word2vec_sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa0ebe",
   "metadata": {},
   "source": [
    "Ahora podemos cargarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a2590d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.data import find\n",
    "\n",
    "# fichero con los vectores en formato texto\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33439b75",
   "metadata": {},
   "source": [
    "Veamos cuáles son las 3 palabras más cercanas a 'university':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7009a8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las 3 palabras más cercanas a \"university\" son [('universities', 0.7003918290138245), ('faculty', 0.6780906915664673), ('undergraduate', 0.6587096452713013)]\n"
     ]
    }
   ],
   "source": [
    "cerca_uni = model.most_similar(positive=['university'], topn = 3)\n",
    "\n",
    "print('Las 3 palabras más cercanas a \"university\" son', cerca_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d889a47f",
   "metadata": {},
   "source": [
    "Obtenemos el vector de una palabra. Esta vez accesible directamente desde el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf45c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00173332, -0.0474043 , -0.022896  ,  0.0407935 ,  0.0435346 ,\n",
       "       -0.0293455 , -0.0235409 , -0.0715902 , -0.0651406 ,  0.0183813 ,\n",
       "       -0.0249921 , -0.125767  ,  0.0343439 , -0.00026957,  0.043857  ,\n",
       "        0.0372462 ,  0.0240246 , -0.015479  ,  0.0217673 ,  0.0311192 ,\n",
       "        0.0628833 ,  0.045147  , -0.0780398 , -0.0391811 ,  0.021606  ,\n",
       "       -0.0175751 , -0.101903  ,  0.030313  ,  0.0622383 , -0.0551438 ,\n",
       "       -0.0515965 , -0.0432121 , -0.0272494 ,  0.0703003 , -0.117382  ,\n",
       "       -0.0435346 ,  0.0317641 ,  0.0870691 , -0.0312804 ,  0.0606259 ,\n",
       "        0.0353114 , -0.137376  ,  0.089004  , -0.00915032,  0.0580461 ,\n",
       "       -0.0372462 , -0.00136046,  0.0480493 ,  0.0515965 ,  0.0883591 ,\n",
       "       -0.00592554,  0.0325703 ,  0.0174944 , -0.0103193 , -0.0538539 ,\n",
       "       -0.0191875 , -0.113512  , -0.0570787 ,  0.03773   , -0.0298292 ,\n",
       "        0.044502  ,  0.0922288 , -0.0164464 ,  0.0428896 ,  0.0580461 ,\n",
       "       -0.0732026 , -0.0039302 ,  0.0464369 , -0.059981  ,  0.0391811 ,\n",
       "        0.0532089 ,  0.0709452 , -0.0699778 ,  0.0419222 , -0.033054  ,\n",
       "        0.045147  ,  0.0419222 ,  0.00505888,  0.0345052 , -0.00919063,\n",
       "       -0.019913  ,  0.00733638,  0.059981  , -0.022251  ,  0.0162045 ,\n",
       "        0.0509516 , -0.0825544 ,  0.0461144 ,  0.0973884 ,  0.0351501 ,\n",
       "        0.110933  , -0.073525  , -0.0412772 , -0.0174944 ,  0.0635282 ,\n",
       "        0.0225735 ,  0.0230572 , -0.017172  ,  0.0477268 ,  0.00148138,\n",
       "       -0.00334571, -0.0490167 ,  0.0719127 , -0.0644957 , -0.128991  ,\n",
       "        0.025637  , -0.189617  , -0.0261207 , -0.0133828 , -0.0619158 ,\n",
       "        0.0838444 ,  0.051919  ,  0.0412772 ,  0.0464369 ,  0.0664305 ,\n",
       "       -0.0193487 ,  0.0557887 , -0.0928738 ,  0.0567562 ,  0.0632057 ,\n",
       "        0.0399873 , -0.0496617 , -0.00229766, -0.0783622 ,  0.0461144 ,\n",
       "       -0.00276122, -0.0155596 ,  0.0483717 , -0.0187844 ,  0.033699  ,\n",
       "        0.0278944 ,  0.0699778 , -0.00604647,  0.166399  , -0.0160433 ,\n",
       "        0.0732026 , -0.0844893 ,  0.0112867 ,  0.0522415 , -0.0111255 ,\n",
       "        0.022251  , -0.0831994 ,  0.0152371 , -0.0240246 ,  0.026282  ,\n",
       "        0.0139472 , -0.00042577,  0.066753  , -0.0432121 , -0.019913  ,\n",
       "        0.012496  ,  0.0448245 ,  0.0369238 , -0.0812645 , -0.0790072 ,\n",
       "       -0.0780398 , -0.0142697 , -0.00959373,  0.0777173 , -0.00128991,\n",
       "       -0.0166076 ,  0.0609484 ,  0.166399  , -0.00220696, -0.011448  ,\n",
       "       -0.0249921 ,  0.0203161 , -0.103838  , -0.0124154 ,  0.0780398 ,\n",
       "       -0.0422447 ,  0.0509516 ,  0.0980334 ,  0.00894877, -0.0564337 ,\n",
       "       -0.07417   ,  0.00079612,  0.0349889 , -0.0109643 ,  0.00481702,\n",
       "       -0.0712677 ,  0.081587  , -0.0245083 , -0.033699  ,  0.0557887 ,\n",
       "       -0.0166076 ,  0.0686879 , -0.0127379 , -0.073525  ,  0.0909389 ,\n",
       "       -0.0196712 , -0.00580461,  0.00592554,  0.0293455 ,  0.0425671 ,\n",
       "        0.0603034 ,  0.0477268 ,  0.0343439 , -0.043857  , -0.107708  ,\n",
       "        0.0012748 , -0.0375687 , -0.0166883 , -0.0432121 ,  0.00729607,\n",
       "       -0.0483717 ,  0.0354726 ,  0.0761049 ,  0.0503066 ,  0.0751374 ,\n",
       "        0.0159627 ,  0.0224122 , -0.0224122 , -0.0851343 ,  0.037085  ,\n",
       "        0.00119922, -0.0396648 , -0.0703003 ,  0.037085  ,  0.0401485 ,\n",
       "        0.0225735 ,  0.0162045 ,  0.0902939 , -0.00099767, -0.0012496 ,\n",
       "        0.00063992,  0.0619158 , -0.0960985 ,  0.0298292 ,  0.0870691 ,\n",
       "       -0.0844893 , -0.0948086 , -0.134796  ,  0.0348277 ,  0.130926  ,\n",
       "       -0.0240246 , -0.0142697 , -0.0209611 , -0.00019903, -0.0380524 ,\n",
       "        0.103838  , -0.073525  ,  0.0448245 , -0.00648988,  0.0506291 ,\n",
       "        0.00118914, -0.0396648 ,  0.0214448 ,  0.0535314 , -0.0551438 ,\n",
       "        0.010803  , -0.00115891,  0.0622383 ,  0.0638507 ,  0.043857  ,\n",
       "        0.0224122 , -0.112867  , -0.0275719 , -0.0973884 , -0.00693328,\n",
       "        0.0293455 ,  0.107063  , -0.121897  , -0.0283781 , -0.0163658 ,\n",
       "        0.0435346 ,  0.0145921 ,  0.118672  ,  0.0709452 ,  0.0877141 ,\n",
       "       -0.0902939 ,  0.0777173 ,  0.03773   ,  0.029023  ,  0.0619158 ,\n",
       "       -0.109643  ,  0.0499841 , -0.0119317 , -0.0393424 ,  0.0915838 ,\n",
       "        0.0357951 , -0.0240246 ,  0.0767498 ,  0.00072558,  0.0786847 ,\n",
       "       -0.082232  , -0.0407935 , -0.052564  , -0.00198526,  0.0480493 ,\n",
       "       -0.00152169, -0.0744925 , -0.125122  , -0.124477  , -0.0272494 ,\n",
       "       -0.0136247 ,  0.0554663 , -0.015479  ,  0.0538539 ,  0.0509516 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['queen']  # Previamente cambiamos la variable \"model\" por ello si intentaramos ejecutar el mismo comando que anteriormene para \"university\":\n",
    "                # model.wv['queen']\n",
    "                # termina dando error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a8e23",
   "metadata": {},
   "source": [
    "Juguemos un poco...\n",
    "\n",
    "¿cuál es la palabra que no encaja en una lista dada?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50bf54ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f7a25",
   "metadata": {},
   "source": [
    "¿qué vector sale si hacemos `'woman'+'king'-'man'`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54c380c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si hacemos \"woman\"+\"king\"-\"man\" obtenemos la palabra queen con una similitud de 0.7118193507194519\n"
     ]
    }
   ],
   "source": [
    "pal_similar = model.most_similar(positive=['woman','king'], negative=['man'], topn = 1)\n",
    "\n",
    "print('Si hacemos \"woman\"+\"king\"-\"man\" obtenemos la palabra', pal_similar[0][0], 'con una similitud de', pal_similar[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb09ed",
   "metadata": {},
   "source": [
    "¿y qué saldrá de la operación `man + daughter - woman`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81034d22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si hacemos \"man\"+\"daughter\"-\"woman\" obtenemos la palabra son con una similitud de 0.8490632772445679\n"
     ]
    }
   ],
   "source": [
    "pal_similar = model.most_similar(positive=['man', 'daughter'], negative=['woman'], topn=1)\n",
    "\n",
    "print('Si hacemos \"man\"+\"daughter\"-\"woman\" obtenemos la palabra', pal_similar[0][0], 'con una similitud de', pal_similar[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe45fcb1",
   "metadata": {},
   "source": [
    "Ojo, lo que nos devuelve es el vector más próximo. Si, por ejemplo, queremos sacar los 3 vectores más próximos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c27827f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si hacemos \"woman\"+\"king\"-\"man\" obtenemos las palabras: queen, monarch y princess;\n",
      "con una similitud de: 0.7118193507194519, 0.6189674139022827 y 0.5902430415153503 correspondientemente.\n"
     ]
    }
   ],
   "source": [
    "pal_similar = model.most_similar(positive=['woman','king'], negative=['man'], topn = 3)\n",
    "\n",
    "print(f'Si hacemos \"woman\"+\"king\"-\"man\" obtenemos las palabras: {pal_similar[0][0]}, {pal_similar[1][0]} y {pal_similar[2][0]};\\ncon una similitud de: {pal_similar[0][1]}, {pal_similar[1][1]} y {pal_similar[2][1]} correspondientemente.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a2d79",
   "metadata": {},
   "source": [
    "Podemos explorar más ejemplos para obtener analogías semánticas menos evidentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32192dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Paris', 'Japan'] - ['France'] → [('Tokyo', 0.8142859935760498), ('Osaka', 0.6350962519645691), ('Seoul', 0.6054925322532654)]\n",
      "['teacher', 'hospital'] - ['school'] → [('Hospital', 0.6331107020378113), ('nurse', 0.6280134916305542), ('doctor', 0.5242676734924316)]\n"
     ]
    }
   ],
   "source": [
    "analogias = [\n",
    "    (['Paris','Japan'], ['France']),\n",
    "    (['teacher','hospital'], ['school'])\n",
    "]\n",
    "\n",
    "for pos, neg in analogias:\n",
    "    print(pos, '-', neg, '→', model.most_similar(positive=pos, negative=neg, topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4857c44b",
   "metadata": {},
   "source": [
    "### Ejercicio 1: Visualización de relaciones semánticas de embeddings mediante PCA\n",
    "\n",
    "Usando el modelo Word2Vec previamente cargado, selecciona varios conjuntos de palabras que incluyan diferentes relaciones semánticas:\n",
    "- Género de varias palabras: masculino, femenino\n",
    "- Varios países y sus capitales\n",
    "- Grados de varios adjetivos: positivo, comparativo, superlativo\n",
    "- Palabras de la varias temáticas, p. ej., alimentos, medios de transporte, etc.\n",
    "\n",
    "Reduce sus embeddings a dos dimensiones mediante PCA (Principal Component Analysis) y representa gráficamente las posiciones relativas de los embeddings de las palabras en el plano.\n",
    "\n",
    "Observa si el modelo organiza de forma coherente las palabras; por ejemplo, king–queen vs. ..., Paris–France vs. ..., good–better–best vs. ..., apple-banana-grape vs. ....\n",
    "\n",
    "##### Discute brevemente los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649aae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TO DO: establecer las palabras y obtener sus vectores (embeddings)\n",
    "\n",
    "# Reducimos a 2D con PCA\n",
    "pca = PCA(n_components=2)\n",
    "coords = pca.fit_transform(vectores)\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(coords[:,0], coords[:,1])\n",
    "for i, palabra in enumerate(palabras):\n",
    "    plt.annotate(palabra, (coords[i,0], coords[i,1]))\n",
    "plt.title(\"Visualización de embeddings Word2Vec con PCA\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe75c7",
   "metadata": {},
   "source": [
    "\n",
    "### Ejercicio 2: Valoración de sesgos en embeddings\n",
    "\n",
    "Usando el modelo Word2Vec cargado, analiza posibles sesgos de género midiendo qué tan similares son ciertas profesiones a las palabras \"man\" y \"woman\".\n",
    "\n",
    "Selecciona una lista de profesiones (p. ej., \"doctor\", \"nurse\", \"engineer\", \"teacher\", \"secretary\", \"scientist\", \"housekeeper\") y calcula su similitud con ambas palabras.\n",
    "\n",
    "Observa si el modelo asocia algunas profesiones más fuertemente con \"man\" o con \"woman\", lo que podría indicar un sesgo aprendido a partir del corpus de entrenamiento.\n",
    "\n",
    "##### Discute brevemente los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3743762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de profesiones a evaluar\n",
    "profesiones = ['doctor', 'nurse', 'engineer', 'teacher', 'secretary', 'scientist', 'housekeeper']\n",
    "\n",
    "# TO DO: comparar las similitudes de cada profesión con 'man' y 'woman'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29c3b2e",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Representación de una oración mediante un embedding\n",
    "\n",
    "Implementa una función `embedding_oracion` que obtenga el vector representativo de una oración calculando el promedio (p. ej., usando `mean` de `numpy`) de los embeddings de sus palabras.\n",
    "\n",
    "Usa esta función para comparar varias oraciones y medir su similitud semántica mediante la distancia coseno (`model.cosine_similarities`) entre sus embeddings.\n",
    "\n",
    "Observa si oraciones con significados parecidos (p. ej., \"The cat sat on the mat\" y \"A dog rested on the rug\") presentan una similitud mayor que las que hablan de temas distintos (p. ej., \"I like pizza and pasta\").\n",
    "\n",
    "##### Discute brevemente los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4942f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def embedding_oracion(oracion):\n",
    "    # TO DO\n",
    "\n",
    "o1 = \"The cat sat on the mat\"\n",
    "o2 = \"A dog rested on the rug\"\n",
    "o3 = \"I like pizza and pasta\"\n",
    "\n",
    "sim_1_2 = model.cosine_similarities(embedding_oracion(o1), [embedding_oracion(o2)])[0]\n",
    "sim_1_3 = model.cosine_similarities(embedding_oracion(o1), [embedding_oracion(o3)])[0]\n",
    "\n",
    "print(f\"Similitud o1-o2: {sim_1_2:.3f}\")\n",
    "print(f\"Similitud o1-o3: {sim_1_3:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643af00a",
   "metadata": {},
   "source": [
    "### Ejercicio 4: Medida de la coherencia semántica de una oración\n",
    "\n",
    "Implementa una función `coherencia_oracion` que calcule la coherencia semántica de una oración midiendo la similitud promedio entre los embeddings de palabras consecutivas.\n",
    "\n",
    "Aplica la función a oraciones con sentido (\"the cat sat on the mat\") y a oraciones absurdas (\"the banana drove a spaceship\") para observar cómo varía el nivel de coherencia calculado por el modelo.\n",
    "\n",
    "##### Discute brevemente los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f7904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherencia_oracion(oracion):\n",
    "    # TO DO\n",
    "\n",
    "print(coherencia_oracion(\"the cat sat on the mat\"))\n",
    "print(coherencia_oracion(\"the banana drove a spaceship\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9303537",
   "metadata": {},
   "source": [
    "### Ejercicio 5: Análisis de la reciprocidad entre vecinos semánticos\n",
    "\n",
    "Implementa una función `vecinos_reciprocos` que evalúe si la relación de similitud entre palabras es recíproca: es decir, si una palabra A considera a B como uno de sus vecinos más cercanos, y B también considera a A dentro de sus vecinos.\n",
    "\n",
    "Aplica esta función a varias palabras y calcula el porcentaje de vecinos recíprocos para cada una.\n",
    "\n",
    "##### Reflexiona sobre los resultados y qué pueden revelar acerca de la estructura del espacio de embeddings y las asimetrías en las relaciones de similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058715cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecinos_reciprocos(palabra, topn=5):\n",
    "    # TO DO\n",
    "\n",
    "for w in ['king', 'dog', 'music']:\n",
    "    print(f\"{w}: {vecinos_reciprocos(w)*100:.1f}% de vecinos son recíprocos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
