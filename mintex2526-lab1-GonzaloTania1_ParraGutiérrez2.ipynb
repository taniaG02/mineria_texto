{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee6c83f-bcf9-4922-ab98-c373452e38c3",
   "metadata": {},
   "source": [
    "<h1>Práctica de Laboratorio: Análisis de Emociones y Sentimiento</h1>\n",
    "<p><strong>Máster en Bioinformática y Biología Computacional</strong><br>\n",
    "<strong>Minería de Texto - Curso 2025-26</strong></p>\n",
    "\n",
    "<p><strong>Integrantes:</strong> [Gonzalo Santana, Tania] y [Parra Gutiérrez, Daniel]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa136629-b813-4af7-8e72-52a439d4bcd1",
   "metadata": {},
   "source": [
    "En este ejercicio, trabajaremos con el Procesamiento del Lenguaje Natural (PLN) para analizar las emociones \n",
    "expresadas en textos literarios disponibles en Project Gutenberg. El objetivo es construir un sistema que pueda \n",
    "identificar y contar las emociones y sentimientos presentes en estas obras. Este ejercicio se enfoca en el uso de \n",
    "técnicas avanzadas de PLN, la extracción de información de texto y el procesamiento de lenguaje natural en \n",
    "general. \n",
    "\n",
    "Para llevar a cabo este ejercicio, se te proporcionará acceso a una serie de recursos y herramientas, incluyendo \n",
    "el léxico de emociones NRC (National Research Council), la base de datos léxica WordNet, y la biblioteca de \n",
    "Python Beautiful Soup. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69c107-5b26-439e-968e-e9f4b6346f1b",
   "metadata": {},
   "source": [
    "#### Configuración inicial\n",
    "En esta sección, importamos las bibliotecas necesarias y descargamos los recursos de NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4351d7-0db5-4d23-8362-64746a610b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee00da74-8579-474b-aabb-50b8c221b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tania\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tania\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Tania\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Tania\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importaciones necesarias\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')  # recursos adicionales si hacen falta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "89249309-2686-4930-9313-5b0ba74d2ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tania\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "520a4184-ac11-4edf-9d52-17fabb6da48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tania\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Tania/nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEste es un ejemplo de tokenización.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Tania/nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Este es un ejemplo de tokenización.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fdd3be-0cbb-4e18-81bf-0a567cf1ab27",
   "metadata": {},
   "source": [
    "**Tarea 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a05eac2-d319-4210-a39b-4717cec07818",
   "metadata": {},
   "source": [
    "(1.5 puntos) Cargar en una estructura de datos Python el Word-Emotion Association Lexicon del \n",
    "NRC5. Asegúrate de entender cómo se estructura el léxico y cómo se mapean las palabras a las \n",
    "emociones. Hay que tener en cuenta que existen varios ficheros con la misma información: un fichero \n",
    "con toda la información, un fichero por emoción, etc. Se puede elegir la opción que se estime oportuna. \n",
    "Se deberá considerar cómo organizar el léxico en memoria para un acceso rápido durante el análisis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df2551-7523-4d94-bc32-c906bd771212",
   "metadata": {},
   "source": [
    "##### Carga del léxico de emociones NRC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6f0c15-452e-47cd-bfb4-46a9161076a0",
   "metadata": {},
   "source": [
    "Vamos a cargar el léxico NRC. Supongamos que hemos descargado el archivo \"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\" desde el enlace proporcionado. Este archivo tiene el formato: palabra, emoción, asociación (1 o 0).\n",
    "\n",
    "Nota: Podemos descargar el archivo manualmente o por código."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a7bfd5-2aa7-423f-834b-231afe1ece81",
   "metadata": {},
   "source": [
    "###### Descarga por código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6087bd40-6bda-4a60-ac6d-327392330014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo descargado y guardado como 'NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL directa del léxico NRC (puede cambiar, pero esta es la habitual)\n",
    "# url = \"https:/mmm/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "\n",
    "# Nombre del archivo local\n",
    "filename = \"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "\n",
    "# Descargar y guardar\n",
    "response = requests.get(url)\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "print(f\"Archivo descargado y guardado como '{filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71372602-da0a-4aa2-998f-c159982599ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ad511367-a1f3-483a-a499-062c0f7b4601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abacus', ['trust']),\n",
       " ('abandon', ['fear', 'negative', 'sadness']),\n",
       " ('abandoned', ['anger', 'fear', 'negative', 'sadness']),\n",
       " ('abandonment', ['anger', 'fear', 'negative', 'sadness', 'surprise']),\n",
       " ('abba', ['positive']),\n",
       " ('abbot', ['trust']),\n",
       " ('abduction', ['fear', 'negative', 'sadness', 'surprise']),\n",
       " ('aberrant', ['negative']),\n",
       " ('aberration', ['disgust', 'negative']),\n",
       " ('abhor', ['anger', 'disgust', 'fear', 'negative'])]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el léxico NRC\n",
    "emolex_file = 'NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "emolex = {}\n",
    "\n",
    "with open(emolex_file, 'r') as f:\n",
    "    for line in f:\n",
    "        word, emotion, association = line.strip().split('\\t')\n",
    "        if association == '1':\n",
    "            if word not in emolex:\n",
    "                emolex[word] = []\n",
    "            emolex[word].append(emotion)\n",
    "\n",
    "# Mostrar algunas palabras y sus emociones\n",
    "list(emolex.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3cc064d2-e6de-488c-8f57-f4fba67c7a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emolex(file_path):\n",
    "    \"\"\"\n",
    "    Carga el léxico de emociones NRC desde un archivo\n",
    "    Retorna: dict con estructura {palabra: [emociones]}\n",
    "    \"\"\"\n",
    "    emolex = {}\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 3:\n",
    "                    word, emotion, value = parts\n",
    "                    if value == '1':\n",
    "                        if word not in emolex:\n",
    "                            emolex[word] = []\n",
    "                        emolex[word].append(emotion)\n",
    "        print(f\"EmoLex cargado: {len(emolex)} palabras\")\n",
    "        return emolex\n",
    "    except FileNotFoundError:\n",
    "        print(\"Archivo EmoLex no encontrado\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d97d84b2-883f-429f-a183-10118ba09d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmoLex cargado: 6453 palabras\n"
     ]
    }
   ],
   "source": [
    "emolex = load_emolex('NRC-Emotion-Lexicon-Wordlevel-v0.92.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e6e69566-5cae-47f0-93da-38320b8842bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_emolex(filepath):\n",
    "    # adapta si usas el fichero \"full\" o los ficheros por emoción\n",
    "    word_emotions = defaultdict(set)  # word -> set(emotions)\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            if len(row) < 3: continue\n",
    "            word, emotion, association = row[0].strip(), row[1].strip(), row[2].strip()\n",
    "            if association == '1':\n",
    "                word_emotions[word].add(emotion)\n",
    "    return word_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2a019728-5b06-4a1e-82ee-90e363af7519",
   "metadata": {},
   "outputs": [],
   "source": [
    "emolex = load_emolex('NRC-Emotion-Lexicon-Wordlevel-v0.92.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5b6b9f64-1d65-4af1-a790-303a0feeefdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'abacus': {'trust'},\n",
       "             'abandon': {'fear', 'negative', 'sadness'},\n",
       "             'abandoned': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'abandonment': {'anger',\n",
       "              'fear',\n",
       "              'negative',\n",
       "              'sadness',\n",
       "              'surprise'},\n",
       "             'abba': {'positive'},\n",
       "             'abbot': {'trust'},\n",
       "             'abduction': {'fear', 'negative', 'sadness', 'surprise'},\n",
       "             'aberrant': {'negative'},\n",
       "             'aberration': {'disgust', 'negative'},\n",
       "             'abhor': {'anger', 'disgust', 'fear', 'negative'},\n",
       "             'abhorrent': {'anger', 'disgust', 'fear', 'negative'},\n",
       "             'ability': {'positive'},\n",
       "             'abject': {'disgust', 'negative'},\n",
       "             'abnormal': {'disgust', 'negative'},\n",
       "             'abolish': {'anger', 'negative'},\n",
       "             'abolition': {'negative'},\n",
       "             'abominable': {'disgust', 'fear', 'negative'},\n",
       "             'abomination': {'anger', 'disgust', 'fear', 'negative'},\n",
       "             'abort': {'negative'},\n",
       "             'abortion': {'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'abortive': {'negative', 'sadness'},\n",
       "             'abovementioned': {'positive'},\n",
       "             'abrasion': {'negative'},\n",
       "             'abrogate': {'negative'},\n",
       "             'abrupt': {'surprise'},\n",
       "             'abscess': {'negative', 'sadness'},\n",
       "             'absence': {'fear', 'negative', 'sadness'},\n",
       "             'absent': {'negative', 'sadness'},\n",
       "             'absentee': {'negative', 'sadness'},\n",
       "             'absenteeism': {'negative'},\n",
       "             'absolute': {'positive'},\n",
       "             'absolution': {'joy', 'positive', 'trust'},\n",
       "             'absorbed': {'positive'},\n",
       "             'absurd': {'negative'},\n",
       "             'absurdity': {'negative'},\n",
       "             'abundance': {'anticipation',\n",
       "              'disgust',\n",
       "              'joy',\n",
       "              'negative',\n",
       "              'positive',\n",
       "              'trust'},\n",
       "             'abundant': {'joy', 'positive'},\n",
       "             'abuse': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'abysmal': {'negative', 'sadness'},\n",
       "             'abyss': {'fear', 'negative', 'sadness'},\n",
       "             'academic': {'positive', 'trust'},\n",
       "             'academy': {'positive'},\n",
       "             'accelerate': {'anticipation'},\n",
       "             'acceptable': {'positive'},\n",
       "             'acceptance': {'positive'},\n",
       "             'accessible': {'positive'},\n",
       "             'accident': {'fear', 'negative', 'sadness', 'surprise'},\n",
       "             'accidental': {'fear', 'negative', 'surprise'},\n",
       "             'accidentally': {'surprise'},\n",
       "             'accolade': {'anticipation',\n",
       "              'joy',\n",
       "              'positive',\n",
       "              'surprise',\n",
       "              'trust'},\n",
       "             'accommodation': {'positive'},\n",
       "             'accompaniment': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'accomplish': {'joy', 'positive'},\n",
       "             'accomplished': {'joy', 'positive'},\n",
       "             'accomplishment': {'positive'},\n",
       "             'accord': {'positive', 'trust'},\n",
       "             'account': {'trust'},\n",
       "             'accountability': {'positive', 'trust'},\n",
       "             'accountable': {'positive', 'trust'},\n",
       "             'accountant': {'trust'},\n",
       "             'accounts': {'trust'},\n",
       "             'accredited': {'positive', 'trust'},\n",
       "             'accueil': {'positive'},\n",
       "             'accurate': {'positive', 'trust'},\n",
       "             'accursed': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'accusation': {'anger', 'disgust', 'negative'},\n",
       "             'accusative': {'negative'},\n",
       "             'accused': {'anger', 'fear', 'negative'},\n",
       "             'accuser': {'anger', 'fear', 'negative'},\n",
       "             'accusing': {'anger', 'fear', 'negative'},\n",
       "             'ace': {'positive'},\n",
       "             'ache': {'negative', 'sadness'},\n",
       "             'achieve': {'joy', 'positive', 'trust'},\n",
       "             'achievement': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'aching': {'negative', 'sadness'},\n",
       "             'acid': {'negative'},\n",
       "             'acknowledgment': {'positive'},\n",
       "             'acquire': {'positive'},\n",
       "             'acquiring': {'anticipation', 'positive'},\n",
       "             'acrobat': {'fear', 'joy', 'positive', 'trust'},\n",
       "             'action': {'positive'},\n",
       "             'actionable': {'anger', 'disgust', 'negative'},\n",
       "             'actual': {'positive'},\n",
       "             'acuity': {'positive'},\n",
       "             'acumen': {'positive'},\n",
       "             'adapt': {'positive'},\n",
       "             'adaptable': {'positive'},\n",
       "             'adder': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'addiction': {'negative'},\n",
       "             'addresses': {'anticipation', 'positive'},\n",
       "             'adept': {'positive'},\n",
       "             'adequacy': {'positive'},\n",
       "             'adhering': {'trust'},\n",
       "             'adipose': {'negative'},\n",
       "             'adjudicate': {'fear', 'negative'},\n",
       "             'adjunct': {'positive'},\n",
       "             'administrative': {'trust'},\n",
       "             'admirable': {'joy', 'positive', 'trust'},\n",
       "             'admiral': {'positive', 'trust'},\n",
       "             'admiration': {'joy', 'positive', 'trust'},\n",
       "             'admire': {'positive', 'trust'},\n",
       "             'admirer': {'positive'},\n",
       "             'admissible': {'positive', 'trust'},\n",
       "             'admonition': {'fear', 'negative'},\n",
       "             'adorable': {'joy', 'positive'},\n",
       "             'adoration': {'joy', 'positive', 'trust'},\n",
       "             'adore': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'adrift': {'anticipation', 'fear', 'negative', 'sadness'},\n",
       "             'adulterated': {'negative'},\n",
       "             'adultery': {'disgust', 'negative', 'sadness'},\n",
       "             'advance': {'anticipation',\n",
       "              'fear',\n",
       "              'joy',\n",
       "              'positive',\n",
       "              'surprise'},\n",
       "             'advanced': {'positive'},\n",
       "             'advancement': {'positive'},\n",
       "             'advantage': {'positive'},\n",
       "             'advantageous': {'positive'},\n",
       "             'advent': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'adventure': {'anticipation', 'positive'},\n",
       "             'adventurous': {'positive'},\n",
       "             'adversary': {'anger', 'negative'},\n",
       "             'adverse': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'adversity': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'advice': {'trust'},\n",
       "             'advisable': {'positive', 'trust'},\n",
       "             'advise': {'positive', 'trust'},\n",
       "             'advised': {'trust'},\n",
       "             'adviser': {'positive', 'trust'},\n",
       "             'advocacy': {'anger', 'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'advocate': {'trust'},\n",
       "             'aesthetic': {'positive'},\n",
       "             'aesthetics': {'joy', 'positive'},\n",
       "             'affable': {'positive'},\n",
       "             'affection': {'joy', 'positive', 'trust'},\n",
       "             'affiliated': {'positive'},\n",
       "             'affirm': {'positive', 'trust'},\n",
       "             'affirmation': {'positive'},\n",
       "             'affirmative': {'positive'},\n",
       "             'affirmatively': {'positive', 'trust'},\n",
       "             'afflict': {'fear', 'negative', 'sadness'},\n",
       "             'afflicted': {'negative'},\n",
       "             'affliction': {'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'affluence': {'joy', 'positive'},\n",
       "             'affluent': {'positive'},\n",
       "             'afford': {'positive'},\n",
       "             'affront': {'anger',\n",
       "              'disgust',\n",
       "              'fear',\n",
       "              'negative',\n",
       "              'sadness',\n",
       "              'surprise'},\n",
       "             'afraid': {'fear', 'negative'},\n",
       "             'aftermath': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'aftertaste': {'negative'},\n",
       "             'aga': {'fear', 'positive', 'trust'},\n",
       "             'aggravated': {'anger', 'negative'},\n",
       "             'aggravating': {'anger', 'negative', 'sadness'},\n",
       "             'aggravation': {'anger', 'disgust', 'negative'},\n",
       "             'aggression': {'anger', 'fear', 'negative'},\n",
       "             'aggressive': {'anger', 'fear', 'negative'},\n",
       "             'aggressor': {'anger', 'fear', 'negative'},\n",
       "             'aghast': {'disgust', 'fear', 'negative', 'surprise'},\n",
       "             'agile': {'positive'},\n",
       "             'agility': {'positive'},\n",
       "             'agitated': {'anger', 'negative'},\n",
       "             'agitation': {'anger', 'negative'},\n",
       "             'agonizing': {'fear', 'negative'},\n",
       "             'agony': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'agree': {'positive'},\n",
       "             'agreeable': {'positive', 'trust'},\n",
       "             'agreed': {'positive', 'trust'},\n",
       "             'agreeing': {'positive', 'trust'},\n",
       "             'agreement': {'positive', 'trust'},\n",
       "             'agriculture': {'positive'},\n",
       "             'aground': {'negative'},\n",
       "             'ahead': {'positive'},\n",
       "             'aid': {'positive'},\n",
       "             'aiding': {'positive'},\n",
       "             'ail': {'negative', 'sadness'},\n",
       "             'ailing': {'fear', 'negative', 'sadness'},\n",
       "             'aimless': {'negative'},\n",
       "             'airport': {'anticipation'},\n",
       "             'airs': {'disgust', 'negative'},\n",
       "             'akin': {'trust'},\n",
       "             'alabaster': {'positive'},\n",
       "             'alarm': {'fear', 'negative', 'surprise'},\n",
       "             'alarming': {'fear', 'negative', 'surprise'},\n",
       "             'alb': {'trust'},\n",
       "             'alcoholism': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'alertness': {'anticipation', 'fear', 'positive', 'surprise'},\n",
       "             'alerts': {'anticipation', 'fear', 'surprise'},\n",
       "             'alien': {'disgust', 'fear', 'negative'},\n",
       "             'alienate': {'anger', 'disgust', 'negative'},\n",
       "             'alienated': {'negative', 'sadness'},\n",
       "             'alienation': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'alimentation': {'positive'},\n",
       "             'alimony': {'negative'},\n",
       "             'alive': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'allay': {'positive'},\n",
       "             'allegation': {'anger', 'negative'},\n",
       "             'allege': {'negative'},\n",
       "             'allegiance': {'positive', 'trust'},\n",
       "             'allegro': {'positive'},\n",
       "             'alleviate': {'positive'},\n",
       "             'alleviation': {'positive'},\n",
       "             'alliance': {'trust'},\n",
       "             'allied': {'positive', 'trust'},\n",
       "             'allowable': {'positive'},\n",
       "             'allure': {'anticipation', 'joy', 'positive', 'surprise'},\n",
       "             'alluring': {'positive'},\n",
       "             'ally': {'positive', 'trust'},\n",
       "             'almighty': {'positive'},\n",
       "             'aloha': {'anticipation', 'joy', 'positive'},\n",
       "             'aloof': {'negative'},\n",
       "             'altercation': {'anger', 'negative'},\n",
       "             'amaze': {'surprise'},\n",
       "             'amazingly': {'joy', 'positive', 'surprise'},\n",
       "             'ambassador': {'positive', 'trust'},\n",
       "             'ambiguous': {'negative'},\n",
       "             'ambition': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'ambulance': {'fear', 'trust'},\n",
       "             'ambush': {'anger', 'fear', 'negative', 'surprise'},\n",
       "             'ameliorate': {'positive'},\n",
       "             'amen': {'joy', 'positive', 'trust'},\n",
       "             'amenable': {'positive'},\n",
       "             'amend': {'positive'},\n",
       "             'amends': {'positive'},\n",
       "             'amenity': {'positive'},\n",
       "             'amiable': {'positive'},\n",
       "             'amicable': {'joy', 'positive'},\n",
       "             'ammonia': {'disgust'},\n",
       "             'amnesia': {'negative'},\n",
       "             'amnesty': {'joy', 'positive'},\n",
       "             'amortization': {'trust'},\n",
       "             'amour': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'amphetamines': {'disgust', 'negative'},\n",
       "             'amuse': {'joy', 'positive'},\n",
       "             'amused': {'joy', 'positive'},\n",
       "             'amusement': {'joy', 'positive'},\n",
       "             'amusing': {'joy', 'positive'},\n",
       "             'anaconda': {'disgust', 'fear', 'negative'},\n",
       "             'anal': {'negative'},\n",
       "             'analyst': {'anticipation', 'positive', 'trust'},\n",
       "             'anarchism': {'anger', 'fear', 'negative'},\n",
       "             'anarchist': {'anger', 'fear', 'negative'},\n",
       "             'anarchy': {'anger', 'fear', 'negative'},\n",
       "             'anathema': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'ancestral': {'trust'},\n",
       "             'anchor': {'positive'},\n",
       "             'anchorage': {'positive', 'sadness'},\n",
       "             'ancient': {'negative'},\n",
       "             'angel': {'anticipation', 'joy', 'positive', 'surprise', 'trust'},\n",
       "             'angelic': {'joy', 'positive', 'trust'},\n",
       "             'anger': {'anger', 'negative'},\n",
       "             'angina': {'fear', 'negative'},\n",
       "             'angling': {'anticipation', 'negative'},\n",
       "             'angry': {'anger', 'disgust', 'negative'},\n",
       "             'anguish': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'animate': {'positive'},\n",
       "             'animated': {'joy', 'positive'},\n",
       "             'animosity': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'animus': {'anger', 'negative'},\n",
       "             'annihilate': {'anger', 'fear', 'negative'},\n",
       "             'annihilated': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'annihilation': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'announcement': {'anticipation'},\n",
       "             'annoy': {'anger', 'disgust', 'negative'},\n",
       "             'annoyance': {'anger', 'disgust', 'negative'},\n",
       "             'annoying': {'anger', 'negative'},\n",
       "             'annul': {'negative'},\n",
       "             'annulment': {'negative', 'sadness'},\n",
       "             'anomaly': {'fear', 'negative', 'surprise'},\n",
       "             'anonymous': {'negative'},\n",
       "             'answerable': {'trust'},\n",
       "             'antagonism': {'anger', 'negative'},\n",
       "             'antagonist': {'anger', 'negative'},\n",
       "             'antagonistic': {'anger', 'disgust', 'negative'},\n",
       "             'anthrax': {'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'antibiotics': {'positive'},\n",
       "             'antichrist': {'anger', 'disgust', 'fear', 'negative'},\n",
       "             'anticipation': {'anticipation'},\n",
       "             'anticipatory': {'anticipation'},\n",
       "             'antidote': {'anticipation', 'positive', 'trust'},\n",
       "             'antifungal': {'positive', 'trust'},\n",
       "             'antipathy': {'anger', 'disgust', 'negative'},\n",
       "             'antiquated': {'negative'},\n",
       "             'antique': {'positive'},\n",
       "             'antiseptic': {'positive', 'trust'},\n",
       "             'antisocial': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'antithesis': {'anger', 'negative'},\n",
       "             'anxiety': {'anger',\n",
       "              'anticipation',\n",
       "              'fear',\n",
       "              'negative',\n",
       "              'sadness'},\n",
       "             'anxious': {'anticipation', 'fear', 'negative'},\n",
       "             'apache': {'fear', 'negative'},\n",
       "             'apathetic': {'negative', 'sadness'},\n",
       "             'apathy': {'negative', 'sadness'},\n",
       "             'aphid': {'disgust', 'negative'},\n",
       "             'aplomb': {'positive'},\n",
       "             'apologetic': {'positive', 'trust'},\n",
       "             'apologize': {'positive', 'sadness', 'trust'},\n",
       "             'apology': {'positive'},\n",
       "             'apostle': {'positive', 'trust'},\n",
       "             'apostolic': {'trust'},\n",
       "             'appalling': {'disgust', 'fear', 'negative'},\n",
       "             'apparition': {'fear', 'surprise'},\n",
       "             'appeal': {'anticipation'},\n",
       "             'appendicitis': {'fear', 'negative', 'sadness'},\n",
       "             'applause': {'joy', 'positive', 'surprise', 'trust'},\n",
       "             'applicant': {'anticipation'},\n",
       "             'appreciation': {'joy', 'positive', 'trust'},\n",
       "             'apprehend': {'fear'},\n",
       "             'apprehension': {'fear', 'negative'},\n",
       "             'apprehensive': {'anticipation', 'fear', 'negative'},\n",
       "             'apprentice': {'trust'},\n",
       "             'approaching': {'anticipation'},\n",
       "             'approbation': {'positive', 'trust'},\n",
       "             'appropriation': {'negative'},\n",
       "             'approval': {'positive'},\n",
       "             'approve': {'joy', 'positive', 'trust'},\n",
       "             'approving': {'positive'},\n",
       "             'apt': {'positive'},\n",
       "             'aptitude': {'positive'},\n",
       "             'arbiter': {'trust'},\n",
       "             'arbitration': {'anticipation'},\n",
       "             'arbitrator': {'trust'},\n",
       "             'archaeology': {'anticipation', 'positive'},\n",
       "             'archaic': {'negative'},\n",
       "             'architecture': {'trust'},\n",
       "             'ardent': {'anticipation', 'joy', 'positive'},\n",
       "             'ardor': {'positive'},\n",
       "             'arduous': {'negative'},\n",
       "             'argue': {'anger', 'negative'},\n",
       "             'argument': {'anger', 'negative'},\n",
       "             'argumentation': {'anger'},\n",
       "             'argumentative': {'negative'},\n",
       "             'arguments': {'anger'},\n",
       "             'arid': {'negative', 'sadness'},\n",
       "             'aristocracy': {'positive'},\n",
       "             'aristocratic': {'positive'},\n",
       "             'armament': {'anger', 'fear'},\n",
       "             'armaments': {'fear', 'negative'},\n",
       "             'armed': {'anger', 'fear', 'negative', 'positive'},\n",
       "             'armor': {'fear', 'positive', 'trust'},\n",
       "             'armored': {'fear'},\n",
       "             'armory': {'trust'},\n",
       "             'aroma': {'positive'},\n",
       "             'arouse': {'anticipation', 'positive'},\n",
       "             'arraignment': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'array': {'positive'},\n",
       "             'arrears': {'negative'},\n",
       "             'arrest': {'negative'},\n",
       "             'arrival': {'anticipation'},\n",
       "             'arrive': {'anticipation'},\n",
       "             'arrogance': {'negative'},\n",
       "             'arrogant': {'anger', 'disgust', 'negative'},\n",
       "             'arsenic': {'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'arson': {'anger', 'fear', 'negative'},\n",
       "             'art': {'anticipation', 'joy', 'positive', 'sadness', 'surprise'},\n",
       "             'articulate': {'positive'},\n",
       "             'articulation': {'positive'},\n",
       "             'artillery': {'fear', 'negative'},\n",
       "             'artisan': {'positive'},\n",
       "             'artiste': {'positive'},\n",
       "             'artistic': {'positive'},\n",
       "             'ascendancy': {'positive'},\n",
       "             'ascent': {'positive'},\n",
       "             'ash': {'negative'},\n",
       "             'ashamed': {'disgust', 'negative', 'sadness'},\n",
       "             'ashes': {'negative', 'sadness'},\n",
       "             'asp': {'fear'},\n",
       "             'aspiration': {'anticipation',\n",
       "              'joy',\n",
       "              'positive',\n",
       "              'surprise',\n",
       "              'trust'},\n",
       "             'aspire': {'anticipation', 'joy', 'positive'},\n",
       "             'aspiring': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'ass': {'negative'},\n",
       "             'assail': {'anger', 'fear', 'negative', 'surprise'},\n",
       "             'assailant': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'assassin': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'assassinate': {'anger', 'fear', 'negative'},\n",
       "             'assassination': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'assault': {'anger', 'fear', 'negative'},\n",
       "             'assembly': {'positive', 'trust'},\n",
       "             'assent': {'positive'},\n",
       "             'asserting': {'positive', 'trust'},\n",
       "             'assessment': {'surprise', 'trust'},\n",
       "             'assessor': {'trust'},\n",
       "             'assets': {'positive'},\n",
       "             'asshole': {'anger', 'disgust', 'negative'},\n",
       "             'assignee': {'trust'},\n",
       "             'assist': {'positive', 'trust'},\n",
       "             'assistance': {'positive'},\n",
       "             'associate': {'positive', 'trust'},\n",
       "             'association': {'trust'},\n",
       "             'assuage': {'positive'},\n",
       "             'assurance': {'positive', 'trust'},\n",
       "             'assure': {'trust'},\n",
       "             'assured': {'positive', 'trust'},\n",
       "             'assuredly': {'trust'},\n",
       "             'astonishingly': {'positive', 'surprise'},\n",
       "             'astonishment': {'joy', 'positive', 'surprise'},\n",
       "             'astray': {'fear', 'negative'},\n",
       "             'astringent': {'negative'},\n",
       "             'astrologer': {'anticipation', 'positive'},\n",
       "             'astronaut': {'positive'},\n",
       "             'astronomer': {'anticipation', 'positive'},\n",
       "             'astute': {'positive'},\n",
       "             'asylum': {'fear', 'negative'},\n",
       "             'asymmetry': {'disgust'},\n",
       "             'atheism': {'negative'},\n",
       "             'atherosclerosis': {'fear', 'negative', 'sadness'},\n",
       "             'athlete': {'positive'},\n",
       "             'athletic': {'positive'},\n",
       "             'atom': {'positive'},\n",
       "             'atone': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'atonement': {'positive'},\n",
       "             'atrocious': {'anger', 'disgust', 'negative'},\n",
       "             'atrocity': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'atrophy': {'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'attachment': {'positive'},\n",
       "             'attack': {'anger', 'fear', 'negative'},\n",
       "             'attacking': {'anger',\n",
       "              'disgust',\n",
       "              'fear',\n",
       "              'negative',\n",
       "              'sadness',\n",
       "              'surprise'},\n",
       "             'attainable': {'anticipation', 'positive'},\n",
       "             'attainment': {'positive'},\n",
       "             'attempt': {'anticipation'},\n",
       "             'attendance': {'anticipation'},\n",
       "             'attendant': {'positive', 'trust'},\n",
       "             'attention': {'positive'},\n",
       "             'attentive': {'positive', 'trust'},\n",
       "             'attenuated': {'negative'},\n",
       "             'attenuation': {'negative', 'sadness'},\n",
       "             'attest': {'positive', 'trust'},\n",
       "             'attestation': {'trust'},\n",
       "             'attorney': {'anger', 'fear', 'positive', 'trust'},\n",
       "             'attraction': {'positive'},\n",
       "             'attractiveness': {'positive'},\n",
       "             'auction': {'anticipation'},\n",
       "             'audacity': {'negative'},\n",
       "             'audience': {'anticipation'},\n",
       "             'auditor': {'fear', 'trust'},\n",
       "             'augment': {'positive'},\n",
       "             'august': {'positive'},\n",
       "             'aunt': {'positive', 'trust'},\n",
       "             'aura': {'positive'},\n",
       "             'auspicious': {'anticipation', 'joy', 'positive'},\n",
       "             'austere': {'fear', 'negative', 'sadness'},\n",
       "             'austerity': {'negative'},\n",
       "             'authentic': {'joy', 'positive', 'trust'},\n",
       "             'authenticate': {'trust'},\n",
       "             'authentication': {'trust'},\n",
       "             'authenticity': {'positive', 'trust'},\n",
       "             'author': {'positive', 'trust'},\n",
       "             'authoritative': {'positive', 'trust'},\n",
       "             'authority': {'positive', 'trust'},\n",
       "             'authorization': {'positive', 'trust'},\n",
       "             'authorize': {'trust'},\n",
       "             'authorized': {'positive'},\n",
       "             'autocratic': {'negative'},\n",
       "             'automatic': {'trust'},\n",
       "             'autopsy': {'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'avalanche': {'fear', 'negative', 'sadness', 'surprise'},\n",
       "             'avarice': {'anger', 'disgust', 'negative'},\n",
       "             'avatar': {'positive'},\n",
       "             'avenger': {'anger', 'negative'},\n",
       "             'averse': {'anger', 'disgust', 'fear', 'negative'},\n",
       "             'aversion': {'anger', 'disgust', 'fear', 'negative'},\n",
       "             'avoid': {'fear', 'negative'},\n",
       "             'avoidance': {'fear', 'negative'},\n",
       "             'avoiding': {'fear'},\n",
       "             'await': {'anticipation'},\n",
       "             'award': {'anticipation', 'joy', 'positive', 'surprise', 'trust'},\n",
       "             'awful': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'awkwardness': {'disgust', 'negative'},\n",
       "             'awry': {'negative'},\n",
       "             'axiom': {'trust'},\n",
       "             'axiomatic': {'trust'},\n",
       "             'ay': {'positive'},\n",
       "             'aye': {'positive'},\n",
       "             'babble': {'negative'},\n",
       "             'babbling': {'negative'},\n",
       "             'baboon': {'disgust', 'negative'},\n",
       "             'baby': {'joy', 'positive'},\n",
       "             'babysitter': {'trust'},\n",
       "             'baccalaureate': {'positive'},\n",
       "             'backbone': {'anger', 'positive', 'trust'},\n",
       "             'backer': {'trust'},\n",
       "             'backward': {'negative'},\n",
       "             'backwards': {'disgust', 'negative'},\n",
       "             'backwater': {'negative', 'sadness'},\n",
       "             'bacteria': {'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'bacterium': {'disgust', 'fear', 'negative'},\n",
       "             'bad': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'badge': {'trust'},\n",
       "             'badger': {'anger', 'negative'},\n",
       "             'badly': {'negative', 'sadness'},\n",
       "             'badness': {'anger', 'disgust', 'fear', 'negative'},\n",
       "             'bailiff': {'fear', 'negative', 'trust'},\n",
       "             'bait': {'fear', 'negative', 'trust'},\n",
       "             'balance': {'positive'},\n",
       "             'balanced': {'positive'},\n",
       "             'bale': {'fear', 'negative'},\n",
       "             'balk': {'negative'},\n",
       "             'ballad': {'positive'},\n",
       "             'ballet': {'positive'},\n",
       "             'ballot': {'anticipation', 'positive', 'trust'},\n",
       "             'balm': {'anticipation', 'joy', 'negative', 'positive'},\n",
       "             'balsam': {'positive'},\n",
       "             'ban': {'negative'},\n",
       "             'bandit': {'negative'},\n",
       "             'bane': {'anger', 'disgust', 'fear', 'negative'},\n",
       "             'bang': {'anger',\n",
       "              'disgust',\n",
       "              'fear',\n",
       "              'negative',\n",
       "              'sadness',\n",
       "              'surprise'},\n",
       "             'banger': {'anger',\n",
       "              'anticipation',\n",
       "              'fear',\n",
       "              'negative',\n",
       "              'surprise'},\n",
       "             'banish': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'banished': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'banishment': {'anger', 'disgust', 'negative', 'sadness'},\n",
       "             'bank': {'trust'},\n",
       "             'banker': {'trust'},\n",
       "             'bankrupt': {'fear', 'negative', 'sadness'},\n",
       "             'bankruptcy': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'banquet': {'anticipation', 'joy', 'positive'},\n",
       "             'banshee': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'baptism': {'positive'},\n",
       "             'baptismal': {'joy', 'positive'},\n",
       "             'barb': {'anger', 'negative'},\n",
       "             'barbarian': {'fear', 'negative'},\n",
       "             'barbaric': {'anger', 'disgust', 'fear', 'negative'},\n",
       "             'barbarism': {'negative'},\n",
       "             'bard': {'positive'},\n",
       "             'barf': {'disgust'},\n",
       "             'bargain': {'positive', 'trust'},\n",
       "             'bark': {'anger', 'negative'},\n",
       "             'barred': {'negative'},\n",
       "             'barren': {'negative', 'sadness'},\n",
       "             'barricade': {'fear', 'negative'},\n",
       "             'barrier': {'anger', 'negative'},\n",
       "             'barrow': {'disgust'},\n",
       "             'bartender': {'trust'},\n",
       "             'barter': {'trust'},\n",
       "             'base': {'trust'},\n",
       "             'baseless': {'negative'},\n",
       "             'basketball': {'anticipation', 'joy', 'positive'},\n",
       "             'bastard': {'disgust', 'negative', 'sadness'},\n",
       "             'bastion': {'anger', 'positive'},\n",
       "             'bath': {'positive'},\n",
       "             'battalion': {'anger'},\n",
       "             'batter': {'anger', 'fear', 'negative'},\n",
       "             'battered': {'fear', 'negative', 'sadness'},\n",
       "             'battery': {'anger', 'negative'},\n",
       "             'battle': {'anger', 'negative'},\n",
       "             'battled': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'battlefield': {'fear', 'negative'},\n",
       "             'bawdy': {'negative'},\n",
       "             'bayonet': {'anger', 'fear', 'negative'},\n",
       "             'beach': {'joy'},\n",
       "             'beam': {'joy', 'positive'},\n",
       "             'beaming': {'anticipation', 'joy', 'positive'},\n",
       "             'bear': {'anger', 'fear'},\n",
       "             'bearer': {'negative'},\n",
       "             'bearish': {'anger', 'fear'},\n",
       "             'beast': {'anger', 'fear', 'negative'},\n",
       "             'beastly': {'disgust', 'fear', 'negative'},\n",
       "             'beating': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'beautification': {'joy', 'positive', 'trust'},\n",
       "             'beautiful': {'joy', 'positive'},\n",
       "             'beautify': {'joy', 'positive'},\n",
       "             'beauty': {'joy', 'positive'},\n",
       "             'bedrock': {'positive', 'trust'},\n",
       "             'bee': {'anger', 'fear'},\n",
       "             'beer': {'joy', 'positive'},\n",
       "             'befall': {'negative'},\n",
       "             'befitting': {'positive'},\n",
       "             'befriend': {'joy', 'positive', 'trust'},\n",
       "             'beg': {'negative', 'sadness'},\n",
       "             'beggar': {'negative', 'sadness'},\n",
       "             'begging': {'negative'},\n",
       "             'begun': {'anticipation'},\n",
       "             'behemoth': {'fear', 'negative'},\n",
       "             'beholden': {'negative'},\n",
       "             'belated': {'negative'},\n",
       "             'believed': {'trust'},\n",
       "             'believer': {'trust'},\n",
       "             'believing': {'positive', 'trust'},\n",
       "             'belittle': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'belligerent': {'anger', 'fear', 'negative'},\n",
       "             'bellows': {'anger'},\n",
       "             'belt': {'anger', 'fear', 'negative'},\n",
       "             'bender': {'negative'},\n",
       "             'benefactor': {'positive', 'trust'},\n",
       "             'beneficial': {'positive'},\n",
       "             'benefit': {'positive'},\n",
       "             'benevolence': {'joy', 'positive', 'trust'},\n",
       "             'benign': {'joy', 'positive'},\n",
       "             'bequest': {'trust'},\n",
       "             'bereaved': {'negative', 'sadness'},\n",
       "             'bereavement': {'negative', 'sadness'},\n",
       "             'bereft': {'negative'},\n",
       "             'berserk': {'anger', 'negative'},\n",
       "             'berth': {'positive'},\n",
       "             'bestial': {'disgust', 'fear', 'negative'},\n",
       "             'betray': {'anger', 'disgust', 'negative', 'sadness', 'surprise'},\n",
       "             'betrayal': {'anger', 'disgust', 'negative', 'sadness'},\n",
       "             'betrothed': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'betterment': {'positive'},\n",
       "             'beverage': {'positive'},\n",
       "             'beware': {'anticipation', 'fear', 'negative'},\n",
       "             'bewildered': {'fear', 'negative', 'surprise'},\n",
       "             'bewilderment': {'fear', 'surprise'},\n",
       "             'bias': {'anger', 'negative'},\n",
       "             'biased': {'negative'},\n",
       "             'biblical': {'positive'},\n",
       "             'bickering': {'anger', 'disgust', 'negative'},\n",
       "             'biennial': {'anticipation'},\n",
       "             'bier': {'fear', 'negative', 'sadness'},\n",
       "             'bigot': {'anger', 'disgust', 'fear', 'negative'},\n",
       "             'bigoted': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'bile': {'anger', 'disgust', 'negative'},\n",
       "             'bilingual': {'positive'},\n",
       "             'biopsy': {'fear', 'negative'},\n",
       "             'birch': {'anger', 'disgust', 'fear', 'negative'},\n",
       "             'birth': {'anticipation', 'fear', 'joy', 'positive', 'trust'},\n",
       "             'birthday': {'anticipation', 'joy', 'positive', 'surprise'},\n",
       "             'birthplace': {'anger', 'negative'},\n",
       "             'bitch': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'bite': {'negative'},\n",
       "             'bitterly': {'anger', 'disgust', 'negative', 'sadness'},\n",
       "             'bitterness': {'anger', 'disgust', 'negative', 'sadness'},\n",
       "             'bizarre': {'negative', 'surprise'},\n",
       "             'blackjack': {'negative'},\n",
       "             'blackmail': {'anger', 'fear', 'negative'},\n",
       "             'blackness': {'fear', 'negative', 'sadness'},\n",
       "             'blame': {'anger', 'disgust', 'negative'},\n",
       "             'blameless': {'positive'},\n",
       "             'bland': {'negative'},\n",
       "             'blanket': {'trust'},\n",
       "             'blasphemous': {'anger', 'disgust', 'negative'},\n",
       "             'blasphemy': {'anger', 'negative'},\n",
       "             'blast': {'anger', 'fear', 'negative', 'surprise'},\n",
       "             'blatant': {'anger', 'disgust', 'negative'},\n",
       "             'blather': {'negative'},\n",
       "             'blaze': {'anger', 'negative'},\n",
       "             'bleak': {'negative', 'sadness'},\n",
       "             'bleeding': {'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'blemish': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'bless': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'blessed': {'joy', 'positive'},\n",
       "             'blessing': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'blessings': {'anticipation',\n",
       "              'joy',\n",
       "              'positive',\n",
       "              'surprise',\n",
       "              'trust'},\n",
       "             'blight': {'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'blighted': {'disgust', 'negative', 'sadness'},\n",
       "             'blinded': {'negative'},\n",
       "             'blindfold': {'anticipation', 'fear', 'surprise'},\n",
       "             'blindly': {'negative', 'sadness'},\n",
       "             'blindness': {'negative', 'sadness'},\n",
       "             'bliss': {'joy', 'positive'},\n",
       "             'blissful': {'joy', 'positive'},\n",
       "             'blister': {'disgust', 'negative'},\n",
       "             'blitz': {'surprise'},\n",
       "             'bloated': {'disgust', 'negative'},\n",
       "             'blob': {'disgust', 'fear', 'negative'},\n",
       "             'blockade': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'bloodless': {'positive'},\n",
       "             'bloodshed': {'anger',\n",
       "              'disgust',\n",
       "              'fear',\n",
       "              'negative',\n",
       "              'sadness',\n",
       "              'surprise'},\n",
       "             'bloodthirsty': {'anger', 'disgust', 'fear', 'negative'},\n",
       "             'bloody': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'bloom': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'blossom': {'joy', 'positive'},\n",
       "             'blot': {'negative'},\n",
       "             'blower': {'negative'},\n",
       "             'blowout': {'negative'},\n",
       "             'blue': {'sadness'},\n",
       "             'blues': {'fear', 'negative', 'sadness'},\n",
       "             'bluff': {'negative'},\n",
       "             'blunder': {'disgust', 'negative', 'sadness'},\n",
       "             'blur': {'negative'},\n",
       "             'blurred': {'negative'},\n",
       "             'blush': {'negative'},\n",
       "             'board': {'anticipation'},\n",
       "             'boast': {'negative', 'positive'},\n",
       "             'boasting': {'negative'},\n",
       "             'bodyguard': {'positive', 'trust'},\n",
       "             'bog': {'negative'},\n",
       "             'bogus': {'anger', 'disgust', 'negative'},\n",
       "             'boil': {'disgust', 'negative'},\n",
       "             'boilerplate': {'negative'},\n",
       "             'boisterous': {'anger',\n",
       "              'anticipation',\n",
       "              'joy',\n",
       "              'negative',\n",
       "              'positive'},\n",
       "             'bold': {'positive'},\n",
       "             'boldness': {'positive'},\n",
       "             'bolster': {'positive'},\n",
       "             'bomb': {'anger', 'fear', 'negative', 'sadness', 'surprise'},\n",
       "             'bombard': {'anger', 'fear', 'negative'},\n",
       "             'bombardment': {'anger', 'fear', 'negative'},\n",
       "             'bombed': {'disgust', 'negative'},\n",
       "             'bomber': {'fear', 'sadness'},\n",
       "             'bonanza': {'joy', 'positive'},\n",
       "             'bondage': {'fear', 'negative', 'sadness'},\n",
       "             'bonds': {'negative'},\n",
       "             'bonne': {'positive'},\n",
       "             'bonus': {'anticipation', 'joy', 'positive', 'surprise'},\n",
       "             'boo': {'negative'},\n",
       "             'booby': {'negative'},\n",
       "             'bookish': {'positive'},\n",
       "             'bookshop': {'positive'},\n",
       "             'bookworm': {'negative', 'positive'},\n",
       "             'boomerang': {'anticipation', 'trust'},\n",
       "             'boon': {'positive'},\n",
       "             'booze': {'negative'},\n",
       "             'bore': {'negative'},\n",
       "             'boredom': {'negative', 'sadness'},\n",
       "             'boring': {'negative'},\n",
       "             'borrower': {'negative'},\n",
       "             'bother': {'negative'},\n",
       "             'bothering': {'anger', 'negative', 'sadness'},\n",
       "             'bottom': {'negative', 'sadness'},\n",
       "             'bottomless': {'fear'},\n",
       "             'bound': {'negative'},\n",
       "             'bountiful': {'anticipation', 'joy', 'positive'},\n",
       "             'bounty': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'bouquet': {'joy', 'positive', 'trust'},\n",
       "             'bout': {'anger', 'negative'},\n",
       "             'bovine': {'disgust', 'negative'},\n",
       "             'bowels': {'disgust'},\n",
       "             'boxing': {'anger'},\n",
       "             'boycott': {'negative'},\n",
       "             'brag': {'negative'},\n",
       "             'brains': {'positive'},\n",
       "             'bran': {'disgust'},\n",
       "             'brandy': {'negative'},\n",
       "             'bravado': {'negative'},\n",
       "             'bravery': {'positive'},\n",
       "             'brawl': {'anger', 'disgust', 'fear', 'negative'},\n",
       "             'brazen': {'anger', 'negative'},\n",
       "             'breach': {'negative'},\n",
       "             'break': {'surprise'},\n",
       "             'breakdown': {'negative'},\n",
       "             'breakfast': {'positive'},\n",
       "             'breakneck': {'negative'},\n",
       "             'breakup': {'negative', 'sadness'},\n",
       "             'bribe': {'negative'},\n",
       "             'bribery': {'disgust', 'negative'},\n",
       "             'bridal': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'bride': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'bridegroom': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'bridesmaid': {'joy', 'positive', 'trust'},\n",
       "             'brigade': {'fear', 'negative'},\n",
       "             'brighten': {'joy', 'positive', 'surprise', 'trust'},\n",
       "             'brightness': {'positive'},\n",
       "             'brilliant': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'brimstone': {'anger', 'fear', 'negative'},\n",
       "             'bristle': {'negative'},\n",
       "             'broadside': {'anticipation', 'negative'},\n",
       "             'brocade': {'positive'},\n",
       "             'broil': {'anger', 'negative'},\n",
       "             'broke': {'fear', 'negative', 'sadness'},\n",
       "             'broken': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'brothel': {'disgust', 'negative'},\n",
       "             'brother': {'positive', 'trust'},\n",
       "             'brotherhood': {'positive', 'trust'},\n",
       "             'brotherly': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'bruise': {'anticipation', 'negative'},\n",
       "             'brunt': {'anger', 'negative'},\n",
       "             'brutal': {'anger', 'fear', 'negative'},\n",
       "             'brutality': {'anger', 'fear', 'negative'},\n",
       "             'brute': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'buck': {'fear', 'negative', 'positive', 'surprise'},\n",
       "             'buddy': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'budget': {'trust'},\n",
       "             'buffet': {'anger', 'negative'},\n",
       "             'bug': {'disgust', 'fear', 'negative'},\n",
       "             'bugaboo': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'bugle': {'anticipation'},\n",
       "             'build': {'positive'},\n",
       "             'building': {'positive'},\n",
       "             'bulbous': {'negative'},\n",
       "             'bulldog': {'positive'},\n",
       "             'bulletproof': {'positive'},\n",
       "             'bully': {'anger', 'fear', 'negative'},\n",
       "             'bum': {'disgust', 'negative', 'sadness'},\n",
       "             'bummer': {'anger', 'disgust', 'negative'},\n",
       "             'bunker': {'fear'},\n",
       "             'buoy': {'positive'},\n",
       "             'burdensome': {'fear', 'negative', 'sadness'},\n",
       "             'bureaucracy': {'negative', 'trust'},\n",
       "             'bureaucrat': {'disgust', 'negative'},\n",
       "             'burglar': {'disgust', 'fear', 'negative'},\n",
       "             'burglary': {'negative'},\n",
       "             'burial': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'buried': {'fear', 'negative', 'sadness'},\n",
       "             'burke': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'burlesque': {'surprise'},\n",
       "             'burnt': {'disgust', 'negative'},\n",
       "             'bursary': {'trust'},\n",
       "             'bury': {'sadness'},\n",
       "             'buss': {'joy', 'positive'},\n",
       "             'busted': {'anger', 'fear', 'negative'},\n",
       "             'butcher': {'anger', 'disgust', 'fear', 'negative'},\n",
       "             'butler': {'positive', 'trust'},\n",
       "             'butt': {'negative'},\n",
       "             'buttery': {'positive'},\n",
       "             'buxom': {'positive'},\n",
       "             'buzz': {'anticipation', 'fear', 'positive'},\n",
       "             'buzzed': {'negative'},\n",
       "             'bye': {'anticipation'},\n",
       "             'bylaw': {'trust'},\n",
       "             'cab': {'positive'},\n",
       "             'cabal': {'fear', 'negative'},\n",
       "             'cabinet': {'positive', 'trust'},\n",
       "             'cable': {'surprise'},\n",
       "             'cacophony': {'anger', 'disgust', 'negative'},\n",
       "             'cad': {'anger', 'disgust', 'negative'},\n",
       "             'cadaver': {'disgust', 'fear', 'negative', 'sadness', 'surprise'},\n",
       "             'cafe': {'positive'},\n",
       "             'cage': {'negative', 'sadness'},\n",
       "             'calamity': {'sadness'},\n",
       "             'calculating': {'negative'},\n",
       "             'calculation': {'anticipation'},\n",
       "             'calculator': {'positive', 'trust'},\n",
       "             'calf': {'joy', 'positive', 'trust'},\n",
       "             'callous': {'anger', 'disgust', 'negative'},\n",
       "             'calls': {'anticipation', 'negative', 'trust'},\n",
       "             'calm': {'positive'},\n",
       "             'camouflage': {'surprise'},\n",
       "             'camouflaged': {'surprise'},\n",
       "             'campaigning': {'anger', 'fear', 'negative'},\n",
       "             'canary': {'positive'},\n",
       "             'cancel': {'negative', 'sadness'},\n",
       "             'cancer': {'anger', 'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'candid': {'anticipation',\n",
       "              'joy',\n",
       "              'positive',\n",
       "              'surprise',\n",
       "              'trust'},\n",
       "             'candidate': {'positive'},\n",
       "             'candied': {'positive'},\n",
       "             'cane': {'anger', 'fear'},\n",
       "             'canker': {'anger', 'disgust', 'negative'},\n",
       "             'cannibal': {'disgust', 'fear', 'negative'},\n",
       "             'cannibalism': {'disgust', 'negative'},\n",
       "             'cannon': {'anger', 'fear', 'negative'},\n",
       "             'canons': {'trust'},\n",
       "             'cap': {'anticipation', 'trust'},\n",
       "             'capitalist': {'positive'},\n",
       "             'captain': {'positive'},\n",
       "             'captivate': {'anticipation',\n",
       "              'joy',\n",
       "              'positive',\n",
       "              'surprise',\n",
       "              'trust'},\n",
       "             'captivating': {'positive'},\n",
       "             'captive': {'fear', 'negative', 'sadness'},\n",
       "             'captivity': {'negative', 'sadness'},\n",
       "             'captor': {'fear', 'negative'},\n",
       "             'capture': {'negative'},\n",
       "             'carcass': {'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'carcinoma': {'fear', 'negative', 'sadness'},\n",
       "             'cardiomyopathy': {'fear', 'negative', 'sadness'},\n",
       "             'career': {'anticipation', 'positive'},\n",
       "             'careful': {'positive'},\n",
       "             'carefully': {'positive'},\n",
       "             'carelessness': {'anger', 'disgust', 'negative'},\n",
       "             'caress': {'positive'},\n",
       "             'caretaker': {'positive', 'trust'},\n",
       "             'caricature': {'negative'},\n",
       "             'caries': {'disgust', 'negative'},\n",
       "             'carnage': {'anger',\n",
       "              'disgust',\n",
       "              'fear',\n",
       "              'negative',\n",
       "              'sadness',\n",
       "              'surprise'},\n",
       "             'carnal': {'negative'},\n",
       "             'carnivorous': {'fear', 'negative'},\n",
       "             'carol': {'joy', 'positive', 'trust'},\n",
       "             'cartel': {'negative'},\n",
       "             'cartridge': {'fear'},\n",
       "             'cascade': {'positive'},\n",
       "             'case': {'fear', 'negative', 'sadness'},\n",
       "             'cash': {'anger',\n",
       "              'anticipation',\n",
       "              'fear',\n",
       "              'joy',\n",
       "              'positive',\n",
       "              'trust'},\n",
       "             'cashier': {'trust'},\n",
       "             'casket': {'fear', 'negative', 'sadness'},\n",
       "             'caste': {'negative'},\n",
       "             'casualty': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'cataract': {'anticipation', 'fear', 'negative', 'sadness'},\n",
       "             'catastrophe': {'anger',\n",
       "              'disgust',\n",
       "              'fear',\n",
       "              'negative',\n",
       "              'sadness',\n",
       "              'surprise'},\n",
       "             'catch': {'surprise'},\n",
       "             'catechism': {'disgust'},\n",
       "             'categorical': {'positive'},\n",
       "             'cater': {'positive'},\n",
       "             'cathartic': {'positive'},\n",
       "             'cathedral': {'joy', 'positive', 'trust'},\n",
       "             'catheter': {'negative'},\n",
       "             'caution': {'anger', 'anticipation', 'fear', 'negative'},\n",
       "             'cautionary': {'fear'},\n",
       "             'cautious': {'anticipation', 'fear', 'positive', 'trust'},\n",
       "             'cautiously': {'fear', 'positive'},\n",
       "             'cede': {'negative'},\n",
       "             'celebrated': {'anticipation', 'joy', 'positive'},\n",
       "             'celebrating': {'anticipation', 'joy', 'positive'},\n",
       "             'celebration': {'anticipation',\n",
       "              'joy',\n",
       "              'positive',\n",
       "              'surprise',\n",
       "              'trust'},\n",
       "             'celebrity': {'anger',\n",
       "              'anticipation',\n",
       "              'disgust',\n",
       "              'joy',\n",
       "              'negative',\n",
       "              'positive',\n",
       "              'surprise',\n",
       "              'trust'},\n",
       "             'celestial': {'anticipation', 'joy', 'positive'},\n",
       "             'cement': {'anticipation', 'trust'},\n",
       "             'cemetery': {'fear', 'negative', 'sadness'},\n",
       "             'censor': {'anger', 'disgust', 'fear', 'negative', 'trust'},\n",
       "             'censure': {'negative'},\n",
       "             'center': {'positive', 'trust'},\n",
       "             'centurion': {'positive'},\n",
       "             'cerebral': {'positive'},\n",
       "             'ceremony': {'joy', 'positive', 'surprise'},\n",
       "             'certainty': {'positive'},\n",
       "             'certify': {'trust'},\n",
       "             'cess': {'disgust', 'negative'},\n",
       "             'cessation': {'negative'},\n",
       "             'chaff': {'anger', 'fear', 'negative'},\n",
       "             'chafing': {'negative'},\n",
       "             'chagrin': {'disgust', 'negative', 'sadness'},\n",
       "             'chairman': {'positive', 'trust'},\n",
       "             'chairwoman': {'positive', 'trust'},\n",
       "             'challenge': {'anger', 'fear', 'negative'},\n",
       "             'champion': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'chance': {'surprise'},\n",
       "             'chancellor': {'trust'},\n",
       "             'change': {'fear'},\n",
       "             'changeable': {'anticipation', 'surprise'},\n",
       "             'chant': {'anger', 'anticipation', 'joy', 'positive', 'surprise'},\n",
       "             'chaos': {'anger', 'fear', 'negative', 'sadness'},\n",
       "             'chaotic': {'anger', 'negative'},\n",
       "             'chaplain': {'trust'},\n",
       "             'charade': {'negative'},\n",
       "             'chargeable': {'fear', 'negative', 'sadness'},\n",
       "             'charger': {'positive'},\n",
       "             'charitable': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'charity': {'joy', 'positive'},\n",
       "             'charm': {'positive'},\n",
       "             'charmed': {'joy', 'negative', 'positive'},\n",
       "             'charming': {'positive'},\n",
       "             'chart': {'trust'},\n",
       "             'chase': {'negative'},\n",
       "             'chasm': {'fear'},\n",
       "             'chastisement': {'negative'},\n",
       "             'chastity': {'anticipation', 'positive', 'trust'},\n",
       "             'chattering': {'positive'},\n",
       "             'chatty': {'negative'},\n",
       "             'cheap': {'negative'},\n",
       "             'cheat': {'anger', 'disgust', 'negative'},\n",
       "             'checklist': {'positive', 'trust'},\n",
       "             'cheer': {'anticipation', 'joy', 'positive', 'surprise', 'trust'},\n",
       "             'cheerful': {'joy', 'positive', 'surprise'},\n",
       "             'cheerfulness': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'cheering': {'joy', 'positive'},\n",
       "             'cheery': {'anticipation', 'joy', 'positive'},\n",
       "             'cheesecake': {'negative'},\n",
       "             'chemist': {'positive', 'trust'},\n",
       "             'cherish': {'anticipation',\n",
       "              'joy',\n",
       "              'positive',\n",
       "              'surprise',\n",
       "              'trust'},\n",
       "             'cherry': {'positive'},\n",
       "             'chicane': {'anticipation', 'negative', 'surprise', 'trust'},\n",
       "             'chicken': {'fear'},\n",
       "             'chieftain': {'positive'},\n",
       "             'child': {'anticipation', 'joy', 'positive'},\n",
       "             'childhood': {'joy', 'positive'},\n",
       "             'childish': {'negative'},\n",
       "             'chilly': {'negative'},\n",
       "             'chimera': {'fear', 'surprise'},\n",
       "             'chirp': {'joy', 'positive'},\n",
       "             'chisel': {'positive'},\n",
       "             'chivalry': {'positive'},\n",
       "             'chloroform': {'negative'},\n",
       "             'chocolate': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'choice': {'positive'},\n",
       "             'choir': {'joy', 'positive', 'trust'},\n",
       "             'choke': {'anger', 'negative', 'sadness'},\n",
       "             'cholera': {'disgust', 'fear', 'negative', 'sadness'},\n",
       "             'chop': {'negative'},\n",
       "             'choral': {'joy', 'positive'},\n",
       "             'chore': {'negative'},\n",
       "             'chorus': {'positive'},\n",
       "             'chosen': {'positive'},\n",
       "             'chowder': {'positive'},\n",
       "             'chronic': {'negative', 'sadness'},\n",
       "             'chronicle': {'positive', 'trust'},\n",
       "             'chuckle': {'anticipation',\n",
       "              'joy',\n",
       "              'positive',\n",
       "              'surprise',\n",
       "              'trust'},\n",
       "             'church': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'cider': {'positive'},\n",
       "             'cigarette': {'negative'},\n",
       "             'circumcision': {'positive'},\n",
       "             'circumvention': {'negative', 'positive'},\n",
       "             'citizen': {'positive'},\n",
       "             'civil': {'positive'},\n",
       "             'civility': {'positive'},\n",
       "             'civilization': {'positive', 'trust'},\n",
       "             'civilized': {'joy', 'positive', 'trust'},\n",
       "             'claimant': {'anger', 'disgust'},\n",
       "             'clairvoyant': {'positive'},\n",
       "             'clamor': {'anger',\n",
       "              'anticipation',\n",
       "              'disgust',\n",
       "              'negative',\n",
       "              'surprise'},\n",
       "             'clan': {'trust'},\n",
       "             'clap': {'anticipation', 'joy', 'positive', 'trust'},\n",
       "             'clarify': {'positive'},\n",
       "             'clash': {'anger', 'negative'},\n",
       "             'clashing': {'anger', 'fear', 'negative'},\n",
       "             'classic': {'positive'},\n",
       "             'classical': {'positive'},\n",
       "             'classics': {'joy', 'positive'},\n",
       "             'classify': {'positive'},\n",
       "             'claw': {'anger', 'fear', 'negative'},\n",
       "             'clean': {'joy', 'positive', 'trust'},\n",
       "             'cleaning': {'positive'},\n",
       "             'cleanliness': {'positive'},\n",
       "             'cleanly': {'positive'},\n",
       "             'cleanse': {'positive'},\n",
       "             'cleansing': {'positive'},\n",
       "             'clearance': {'positive', 'trust'},\n",
       "             'clearness': {'positive'},\n",
       "             'cleave': {'fear'},\n",
       "             'clerical': {'positive', 'trust'},\n",
       "             'clever': {'positive'},\n",
       "             'cleverness': {'positive'},\n",
       "             'cliff': {'fear'},\n",
       "             'climax': {'anticipation',\n",
       "              'joy',\n",
       "              'positive',\n",
       "              'surprise',\n",
       "              'trust'},\n",
       "             'clock': {'anticipation'},\n",
       "             'cloister': {'negative'},\n",
       "             'closeness': {'joy', 'positive', 'trust'},\n",
       "             'closure': {'anticipation', 'joy', 'positive', 'sadness'},\n",
       "             'clothe': {'positive'},\n",
       "             'clouded': {'negative', 'sadness'},\n",
       "             ...})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emolex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187392d9-fed3-49e9-b2ed-3e2ee058a409",
   "metadata": {},
   "source": [
    "#### Extensión del léxico usando WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcab796c-ade9-4d5d-9ea5-98c13567db66",
   "metadata": {},
   "source": [
    "Vamos a extender el léxico NRC añadiendo sinónimos, hipónimos, hiperónimos y formas derivadas de las palabras originales. Usaremos WordNet y la función derivationally_related_forms().\n",
    "\n",
    "Además, usaremos los diccionarios de mapeo de POS-tags proporcionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "278f7a7d-cfcc-4a99-9284-58341de5fd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del léxico extendido: 61842\n"
     ]
    }
   ],
   "source": [
    "# Diccionarios para mapeo de POS-tags\n",
    "wordnet_to_penn = {\n",
    "    'n': 'NN',  # sustantivo\n",
    "    'v': 'VB',  # verbo\n",
    "    'a': 'JJ',  # adjetivo\n",
    "    's': 'JJ',  # adjetivo superlativo\n",
    "    'r': 'RB',  # adverbio\n",
    "    'c': 'CC'   # conjunción\n",
    "}\n",
    "\n",
    "penn_to_wordnet = {\n",
    "    'CC': 'c',   # Coordinating conjunction\n",
    "    'CD': 'c',   # Cardinal number\n",
    "    'DT': 'c',   # Determiner\n",
    "    'EX': 'c',   # Existential there\n",
    "    'FW': 'x',   # Foreign word\n",
    "    'IN': 'c',   # Preposition or subordinating conjunction\n",
    "    'JJ': 'a',   # Adjective\n",
    "    'JJR': 'a',  # Adjective, comparative\n",
    "    'JJS': 'a',  # Adjective, superlative\n",
    "    'LS': 'c',   # List item marker\n",
    "    'MD': 'v',   # Modal\n",
    "    'NN': 'n',   # Noun, singular or mass\n",
    "    'NNS': 'n',  # Noun, plural\n",
    "    'NNP': 'n',  # Proper noun, singular\n",
    "    'NNPS': 'n', # Proper noun, plural\n",
    "    'PDT': 'c',  # Predeterminer\n",
    "    'POS': 'c',  # Possessive ending\n",
    "    'PRP': 'n',  # Personal pronoun\n",
    "    'PRP$': 'n', # Possessive pronoun\n",
    "    'RB': 'r',   # Adverb\n",
    "    'RBR': 'r',  # Adverb, comparative\n",
    "    'RBS': 'r',  # Adverb, superlative\n",
    "    'RP': 'r',   # Particle\n",
    "    'SYM': 'x',  # Symbol\n",
    "    'TO': 'c',   # to\n",
    "    'UH': 'x',   # Interjection\n",
    "    'VB': 'v',   # Verb, base form\n",
    "    'VBD': 'v',  # Verb, past tense\n",
    "    'VBG': 'v',  # Verb, gerund or present participle\n",
    "    'VBN': 'v',  # Verb, past participle\n",
    "    'VBP': 'v',  # Verb, non-3rd person singular present\n",
    "    'VBZ': 'v',  # Verb, 3rd person singular present\n",
    "    'WDT': 'c',  # Wh-determiner\n",
    "    'WP': 'n',   # Wh-pronoun\n",
    "    'WP$': 'n',  # Possessive wh-pronoun\n",
    "    'WRB': 'r',  # Wh-adverb\n",
    "    'X': 'x'     # Any word not categorized by the other tags\n",
    "}\n",
    "\n",
    "# Inicializar el lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Crear un diccionario extendido\n",
    "extended_emolex = {}\n",
    "\n",
    "# Función para obtener sinónimos, hipónimos, hiperónimos y formas derivadas\n",
    "def get_wordnet_relations(word, pos_tag_wn):\n",
    "    synsets = wn.synsets(word, pos=pos_tag_wn)\n",
    "    relations = set()\n",
    "    for synset in synsets:\n",
    "        # Lemas del synset (sinónimos)\n",
    "        for lemma in synset.lemmas():\n",
    "            relations.add(lemma.name().replace('_', ' '))\n",
    "        # Hipónimos\n",
    "        for hypo in synset.hyponyms():\n",
    "            for lemma in hypo.lemmas():\n",
    "                relations.add(lemma.name().replace('_', ' '))\n",
    "        # Hiperónimos\n",
    "        for hyper in synset.hypernyms():\n",
    "            for lemma in hyper.lemmas():\n",
    "                relations.add(lemma.name().replace('_', ' '))\n",
    "        # Formas derivadas (derivationally_related_forms)\n",
    "        for lemma in synset.lemmas():\n",
    "            for related_form in lemma.derivationally_related_forms():\n",
    "                relations.add(related_form.name().replace('_', ' '))\n",
    "    return relations\n",
    "\n",
    "# Recorrer las palabras originales del EmoLex\n",
    "for word, emotions in emolex.items():\n",
    "    # Para cada palabra, intentar obtener su POS-tag usando WordNet\n",
    "    # Inicialmente, no sabemos el POS-tag, así que probaremos con los cuatro principales: n, v, a, r\n",
    "    for pos_wn in ['n', 'v', 'a', 'r']:\n",
    "        # Obtener relaciones\n",
    "        relations = get_wordnet_relations(word, pos_wn)\n",
    "        # Para cada palabra relacionada, agregar las emociones\n",
    "        for related_word in relations:\n",
    "            # Convertir el POS-tag de WordNet a Penn\n",
    "            pos_penn = wordnet_to_penn.get(pos_wn, None)\n",
    "            if pos_penn:\n",
    "                key = (related_word, pos_penn)\n",
    "                if key not in extended_emolex:\n",
    "                    extended_emolex[key] = set()\n",
    "                extended_emolex[key].update(emotions)\n",
    "\n",
    "    # También agregar la palabra original (sin POS-tag específico) pero necesitamos asignarle un POS-tag\n",
    "    # Para la palabra original, usaremos el primer POS-tag que encontremos en WordNet\n",
    "    synsets = wn.synsets(word)\n",
    "    if synsets:\n",
    "        pos_wn = synsets[0].pos()\n",
    "        pos_penn = wordnet_to_penn.get(pos_wn, None)\n",
    "        if pos_penn:\n",
    "            key = (word, pos_penn)\n",
    "            if key not in extended_emolex:\n",
    "                extended_emolex[key] = set()\n",
    "            extended_emolex[key].update(emotions)\n",
    "\n",
    "# Convertir los sets a listas para consistencia\n",
    "for key in extended_emolex:\n",
    "    extended_emolex[key] = list(extended_emolex[key])\n",
    "\n",
    "# Mostrar el tamaño del léxico extendido\n",
    "print(f\"Tamaño del léxico extendido: {len(extended_emolex)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "96665655-dfb1-4652-8693-ff247ee3939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_emolex(original_emolex):\n",
    "    \"\"\"\n",
    "    Extiende el léxico EmoLex usando WordNet\n",
    "    Retorna: dict con estructura {(lemma, pos_tag): [emociones]}\n",
    "    \"\"\"\n",
    "    extended_lexicon = {}\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for word, emotions in original_emolex.items():\n",
    "        # Probar diferentes POS tags para encontrar synsets\n",
    "        for pos in ['n', 'v', 'a', 'r']:\n",
    "            synsets = wn.synsets(word, pos=pos)\n",
    "            \n",
    "            for synset in synsets:\n",
    "                # Añadir lema principal\n",
    "                lemma = lemmatizer.lemmatize(word, pos=pos)\n",
    "                pos_tag_penn = wordnet_to_penn.get(pos, 'NN')\n",
    "                key = (lemma, pos_tag_penn)\n",
    "                \n",
    "                if key not in extended_lexicon:\n",
    "                    extended_lexicon[key] = set()\n",
    "                extended_lexicon[key].update(emotions)\n",
    "                \n",
    "                # Añadir sinónimos\n",
    "                for lemma_obj in synset.lemmas():\n",
    "                    synonym = lemma_obj.name().replace('_', ' ')\n",
    "                    synonym_key = (synonym, pos_tag_penn)\n",
    "                    if synonym_key not in extended_lexicon:\n",
    "                        extended_lexicon[synonym_key] = set()\n",
    "                    extended_lexicon[synonym_key].update(emotions)\n",
    "                \n",
    "                # Añadir hipónimos\n",
    "                for hyponym in synset.hyponyms():\n",
    "                    for lemma_obj in hyponym.lemmas():\n",
    "                        hyponym_word = lemma_obj.name().replace('_', ' ')\n",
    "                        hyponym_key = (hyponym_word, pos_tag_penn)\n",
    "                        if hyponym_key not in extended_lexicon:\n",
    "                            extended_lexicon[hyponym_key] = set()\n",
    "                        extended_lexicon[hyponym_key].update(emotions)\n",
    "                \n",
    "                # Añadir hiperónimos\n",
    "                for hypernym in synset.hypernyms():\n",
    "                    for lemma_obj in hypernym.lemmas():\n",
    "                        hypernym_word = lemma_obj.name().replace('_', ' ')\n",
    "                        hypernym_key = (hypernym_word, pos_tag_penn)\n",
    "                        if hypernym_key not in extended_lexicon:\n",
    "                            extended_lexicon[hypernym_key] = set()\n",
    "                        extended_lexicon[hypernym_key].update(emotions)\n",
    "    \n",
    "    # Convertir sets a listas\n",
    "    return {key: list(emotions) for key, emotions in extended_lexicon.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dbcf67f1-3c12-41aa-8eab-4b20d77f1b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Léxico extendido: 51100 entradas\n",
      "Ejemplo de entradas extendidas: {('abacus', 'NN'): ['trust', 'positive'], ('tablet', 'NN'): ['trust', 'positive'], ('calculator', 'NN'): ['disgust', 'trust', 'negative', 'fear', 'anger', 'positive', 'sadness'], ('calculating machine', 'NN'): ['disgust', 'trust', 'negative', 'fear', 'anger', 'positive', 'sadness'], ('abandon', 'NN'): ['negative', 'trust', 'fear', 'joy', 'anticipation', 'sadness', 'positive']}\n"
     ]
    }
   ],
   "source": [
    "# Extender el léxico\n",
    "extended_emolex = extend_emolex(emolex)\n",
    "print(f\"Léxico extendido: {len(extended_emolex)} entradas\")\n",
    "print(\"Ejemplo de entradas extendidas:\", dict(list(extended_emolex.items())[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d712239-be46-42b5-89eb-2c5539c2629b",
   "metadata": {},
   "source": [
    "#### Descarga de novelas de Project Gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e518f2-1cb8-49db-80bb-d083b8d14bb8",
   "metadata": {},
   "source": [
    "Usamos la función download_text proporcionada para descargar las novelas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d5275fd4-27a1-4690-a6e5-586b28c8f8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando Crime and Punishment...\n",
      "Crime and Punishment descargado correctamente.\n",
      "Descargando War and Peace...\n",
      "War and Peace descargado correctamente.\n",
      "Descargando Pride and Prejudice...\n",
      "Pride and Prejudice descargado correctamente.\n",
      "Descargando Frankenstein...\n",
      "Frankenstein descargado correctamente.\n",
      "Descargando The Adventures of Sherlock Holmes...\n",
      "The Adventures of Sherlock Holmes descargado correctamente.\n",
      "Descargando Ulysses...\n",
      "Ulysses descargado correctamente.\n",
      "Descargando The Odyssey...\n",
      "The Odyssey descargado correctamente.\n",
      "Descargando Moby Dick...\n",
      "Moby Dick descargado correctamente.\n",
      "Descargando The Divine Comedy...\n",
      "The Divine Comedy descargado correctamente.\n",
      "Descargando Critias...\n",
      "Critias descargado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Diccionario de libros\n",
    "books = {\n",
    "    'Crime and Punishment': 'http://www.gutenberg.org/files/2554/2554-0.txt',\n",
    "    'War and Peace': 'http://www.gutenberg.org/files/2600/2600-0.txt',\n",
    "    'Pride and Prejudice': 'http://www.gutenberg.org/files/1342/1342-0.txt',\n",
    "    'Frankenstein': 'https://www.gutenberg.org/cache/epub/84/pg84.txt',\n",
    "    'The Adventures of Sherlock Holmes': 'http://www.gutenberg.org/files/1661/1661-0.txt',\n",
    "    'Ulysses': 'http://www.gutenberg.org/files/4300/4300-0.txt',\n",
    "    'The Odyssey': 'https://www.gutenberg.org/cache/epub/1727/pg1727.txt',\n",
    "    'Moby Dick': 'http://www.gutenberg.org/files/15/15-0.txt',\n",
    "    'The Divine Comedy': 'https://www.gutenberg.org/cache/epub/8800/pg8800.txt',\n",
    "    'Critias': 'https://www.gutenberg.org/cache/epub/1571/pg1571.txt'\n",
    "}\n",
    "\n",
    "def download_text(url):\n",
    "    \"\"\"Descarga el texto de una novela desde Project Gutenberg\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al descargar: {e}\")\n",
    "        return None\n",
    "        \n",
    "# Descargar todos los libros\n",
    "book_texts = {}\n",
    "for title, url in books.items():\n",
    "    print(f\"Descargando {title}...\")\n",
    "    text = download_text(url)\n",
    "    if text:\n",
    "        book_texts[title] = text\n",
    "        print(f\"{title} descargado correctamente.\")\n",
    "    else:\n",
    "        print(f\"Fallo en la descarga de {title}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44061dd-f7ff-44e5-be12-b2ae0374ba1d",
   "metadata": {},
   "source": [
    "#### Análisis de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b004445-7e2b-44f8-a6df-19f18c1dbaa0",
   "metadata": {},
   "source": [
    "Implementamos la función para analizar el texto y contar las emociones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "464b2a5e-7a17-4247-9268-4b7ed95c8d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analizando Crime and Punishment...\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Tania/nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m title, text \u001b[38;5;129;01min\u001b[39;00m book_texts\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalizando \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m     results[title] \u001b[38;5;241m=\u001b[39m analyze_emotions(text, extended_emolex)\n",
      "Cell \u001b[1;32mIn[102], line 6\u001b[0m, in \u001b[0;36manalyze_emotions\u001b[1;34m(text, lexicon)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_emotions\u001b[39m(text, lexicon):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Tokenización\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# POS-tagging\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     pos_tags \u001b[38;5;241m=\u001b[39m pos_tag(tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Tania/nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def analyze_emotions(text, lexicon):\n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(text)\n",
    "    # POS-tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    # Inicializar contador de emociones\n",
    "    emotion_counts = {}\n",
    "    # Procesar cada token\n",
    "    for word, pos_tag_pen in pos_tags:\n",
    "        # Convertir POS-tag de PennTreeBank a WordNet\n",
    "        pos_wn = penn_to_wordnet.get(pos_tag_pen, None)\n",
    "        if pos_wn is None:\n",
    "            continue\n",
    "        # Lematización\n",
    "        lemma = lemmatizer.lemmatize(word, pos=pos_wn)\n",
    "        # Buscar en el léxico extendido\n",
    "        key = (lemma, pos_tag_pen)\n",
    "        if key in lexicon:\n",
    "            emotions = lexicon[key]\n",
    "            for emotion in emotions:\n",
    "                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n",
    "    return emotion_counts\n",
    "\n",
    "# Analizar cada libro\n",
    "results = {}\n",
    "for title, text in book_texts.items():\n",
    "    print(f\"Analizando {title}...\")\n",
    "    results[title] = analyze_emotions(text, extended_emolex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424c7f6-d9f8-4c03-aa80-46f66c6125ff",
   "metadata": {},
   "source": [
    "#### Presentación de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afadcdd-023a-4fa6-bba8-a6ca745acf6e",
   "metadata": {},
   "source": [
    "Vamos a mostrar los resultados en forma de tabla y gráficos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d6508f50-1431-4ca3-a323-b387d27f268c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no numeric data to plot",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Graficar los resultados\u001b[39;00m\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m---> 10\u001b[0m df\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m, stacked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmociones en novelas clásicas\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrecuencia\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\plotting\\_core.py:1030\u001b[0m, in \u001b[0;36mPlotAccessor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1027\u001b[0m             label_name \u001b[38;5;241m=\u001b[39m label_kw \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   1028\u001b[0m             data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m label_name\n\u001b[1;32m-> 1030\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m plot_backend\u001b[38;5;241m.\u001b[39mplot(data, kind\u001b[38;5;241m=\u001b[39mkind, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\plotting\\_matplotlib\\__init__.py:71\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(data, kind, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124max\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(ax, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_ax\u001b[39m\u001b[38;5;124m\"\u001b[39m, ax)\n\u001b[0;32m     70\u001b[0m plot_obj \u001b[38;5;241m=\u001b[39m PLOT_CLASSES[kind](data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 71\u001b[0m plot_obj\u001b[38;5;241m.\u001b[39mgenerate()\n\u001b[0;32m     72\u001b[0m plot_obj\u001b[38;5;241m.\u001b[39mdraw()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m plot_obj\u001b[38;5;241m.\u001b[39mresult\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:499\u001b[0m, in \u001b[0;36mMPLPlot.generate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_plot_data()\n\u001b[0;32m    500\u001b[0m     fig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfig\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_plot(fig)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\plotting\\_matplotlib\\core.py:698\u001b[0m, in \u001b[0;36mMPLPlot._compute_plot_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;66;03m# no non-numeric frames or series allowed\u001b[39;00m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_empty:\n\u001b[1;32m--> 698\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno numeric data to plot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m numeric_data\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_convert_to_ndarray)\n",
      "\u001b[1;31mTypeError\u001b[0m: no numeric data to plot"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear un DataFrame con los resultados\n",
    "df = pd.DataFrame.from_dict(results, orient='index')\n",
    "# Rellenar NaN con 0\n",
    "df = df.fillna(0)\n",
    "# Mostrar la tabla\n",
    "df\n",
    "\n",
    "# Graficar los resultados\n",
    "plt.figure(figsize=(12, 8))\n",
    "df.plot(kind='bar', stacked=True, figsize=(12,8))\n",
    "plt.title('Emociones en novelas clásicas')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.xlabel('Novelas')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa37ab5-a621-493d-bf81-5226723ca67f",
   "metadata": {},
   "source": [
    "#### Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52fdb5-75d5-48df-b9d5-fab801e3f76c",
   "metadata": {},
   "source": [
    "En esta sección, comenta brevemente los resultados obtenidos.\n",
    "* ¿Qué emociones son las más frecuentes en general?\n",
    "* ¿Hay novelas que destacan por alguna emoción en particular?\n",
    "* ¿Qué limitaciones has encontrado?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9948e-19e1-4191-a721-f68c7c85b2f2",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8210d636-9043-400a-b7bd-036a400346ba",
   "metadata": {},
   "source": [
    "# Introducción \n",
    "En esta práctica realizaremos un análisis de emociones en textos literarios clásicos disponibles en Project Gutenberg. Utilizaremos el léxico NRC EmoLex extendido con WordNet para identificar y contar emociones expresadas en las obras.\n",
    "\n",
    "## Instalación y carga de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004047f4-aecd-42b7-9e72-8bea7d4c56d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de dependencias (ejecutar si es necesario)\n",
    "# !pip install nltk requests matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313466a1-5ec6-4d87-a821-e59ad13244aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas cargadas correctamente\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)   ## Añadido por dani para ¿borrar chunk de abajo?\n",
    "\n",
    "print(\"Bibliotecas cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef51aae-9ae9-455e-9f17-68cbe8eed472",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BORRAR ¿?\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')  # <-- este es el que falta\n",
    "nltk.download('averaged_perceptron_tagger')  # para POS tagging\n",
    "nltk.download('wordnet')  # para lematización\n",
    "nltk.download('omw-1.4')  # para soporte multilingüe de WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ef804-ea7a-479a-9408-4022e3c8e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BORRAR ¿?\n",
    "\n",
    "import nltk\n",
    "\n",
    "resources = ['punkt', 'punkt_tab', 'averaged_perceptron_tagger', 'wordnet', 'omw-1.4']\n",
    "\n",
    "for resource in resources:\n",
    "    try:\n",
    "        nltk.data.find(resource)\n",
    "    except LookupError:\n",
    "        nltk.download(resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96bb15d-ef29-4d62-a58e-0e19209ca5fa",
   "metadata": {},
   "source": [
    "## Tarea 1: Cargar el léxico NRC EmoLex (1.5 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac778c-deb0-4598-9109-c470946b9c5a",
   "metadata": {},
   "source": [
    "Cargamos el Word-Emotion Association Lexicon del NRC, que contiene 14,182 palabras asociadas con 8 emociones y 2 sentimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76702b20-7ee6-4f76-ab77-46882f7b56bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este sí que lo pondría como código (osea, no como función)\n",
    "# y podríamos añadir lo de borrar aquellos que sean todo 0 (sin asociación con emoción)\n",
    "\n",
    "# Si quieres puedo copiar el código abajo y lo pruebo (incluyendo la parte de la ampliación del diccionario)\n",
    "# de ese modo podemos comparar el tiempo y las dimensiones de los diccionarios.\n",
    "\n",
    "def load_nrc_lexicon_from_file(filepath):\n",
    "    \"\"\"\n",
    "    Carga el léxico NRC desde el archivo oficial.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Ruta al archivo NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario {palabra: [lista_de_emociones]}\n",
    "    \"\"\"\n",
    "    nrc_lexicon = defaultdict(list)\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 3:\n",
    "                    word, emotion, association = parts\n",
    "                    if association == '1':\n",
    "                        nrc_lexicon[word].append(emotion)\n",
    "        \n",
    "        print(f\"Léxico NRC cargado desde archivo con {len(nrc_lexicon)} palabras\")\n",
    "        return dict(nrc_lexicon)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Archivo no encontrado: {filepath}\")\n",
    "        return load_nrc_lexicon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c917cbbe-fe06-427e-8769-ee4d474683a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Léxico NRC cargado desde archivo con 6453 palabras\n"
     ]
    }
   ],
   "source": [
    "# Cargar el léxico\n",
    "nrc_lexicon = load_nrc_lexicon_from_file('NRC-Emotion-Lexicon-Wordlevel-v0.92.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7283413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Léxico NRC cargado desde archivo con 6453 palabras\n"
     ]
    }
   ],
   "source": [
    "# Versión Dani\n",
    "\n",
    "# Carga el léxico NRC desde el archivo oficial: NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\n",
    "# Nos devolverá un Diccionario {palabra: [lista_de_emociones]}\n",
    "\n",
    "archivo = 'NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "\n",
    "# Preparamos el diccionario\n",
    "nrc_lexicon = defaultdict(list) \n",
    "\n",
    "try:\n",
    "    with open(archivo, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 3:\n",
    "                word, emotion, association = parts\n",
    "                if association == '1':\n",
    "                    nrc_lexicon[word].append(emotion)  # De esta manera almacenamos las emociones en las que sí existe asociación\n",
    "    \n",
    "    print(f\"Léxico NRC cargado desde archivo con {len(nrc_lexicon)} palabras\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Archivo no encontrado: {archivo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e379979-a70c-4f02-b297-4f4a94695e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo de entradas del léxico:\n",
      "  abacus: ['trust']\n",
      "  abandon: ['fear', 'negative', 'sadness']\n",
      "  abandoned: ['anger', 'fear', 'negative', 'sadness']\n",
      "  abandonment: ['anger', 'fear', 'negative', 'sadness', 'surprise']\n",
      "  abba: ['positive']\n"
     ]
    }
   ],
   "source": [
    "# Mostrar ejemplo\n",
    "print(\"\\nEjemplo de entradas del léxico:\")\n",
    "for word in list(nrc_lexicon.keys())[:5]:\n",
    "    print(f\"  {word}: {nrc_lexicon[word]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89732ff4-689b-49e5-be43-29947d45efb9",
   "metadata": {},
   "source": [
    "## Tarea 2: Extender EmoLex con WordNet (3.0 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65335445-a4b5-41ab-a7e7-f57654ed55e5",
   "metadata": {},
   "source": [
    "Extenderemos el léxico usando sinónimos, hipónimos, hiperónimos y formas derivadas de WordNet. El léxico extendido usará claves (lemma, POS-tag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "143521cc-2872-4b80-8282-29ea988bbdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionarios de conversión entre POS tags\n",
    "wordnet_to_penn = {\n",
    "    'n': 'NN',  # sustantivo\n",
    "    'v': 'VB',  # verbo\n",
    "    'a': 'JJ',  # adjetivo\n",
    "    's': 'JJS', # adjetivo superlativo (le añadimos la \"S\") y cambiamos la 'a' por 's'\n",
    "    'r': 'RB',  # adverbio\n",
    "    'c': 'CC'   # conjunción\n",
    "}\n",
    "\n",
    "penn_to_wordnet = {\n",
    "    'CC': 'c', 'CD': 'c', 'DT': 'c', 'EX': 'c', 'FW': 'x', 'IN': 'c',\n",
    "    'JJ': 'a', 'JJR': 'a', 'JJS': 's', 'LS': 'c', 'MD': 'v',            # Cambiamos el 'JJS': 'a' por 'JJS': 's'\n",
    "    'NN': 'n', 'NNS': 'n', 'NNP': 'n', 'NNPS': 'n',\n",
    "    'PDT': 'c', 'POS': 'c', 'PRP': 'n', 'PRP$': 'n',\n",
    "    'RB': 'r', 'RBR': 'r', 'RBS': 'r', 'RP': 'r',\n",
    "    'SYM': 'x', 'TO': 'c', 'UH': 'x',\n",
    "    'VB': 'v', 'VBD': 'v', 'VBG': 'v', 'VBN': 'v', 'VBP': 'v', 'VBZ': 'v',\n",
    "    'WDT': 'c', 'WP': 'n', 'WP$': 'n', 'WRB': 'r', 'X': 'x'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac42bb8d-ec55-4cb0-b4a0-566db140e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta quizás también la podemos poner como código ya que sólo se ejecutaría una vez para ampliar el diccionario\n",
    "\n",
    "def extend_lexicon_with_wordnet(nrc_lexicon):\n",
    "    \"\"\"\n",
    "    Extiende el léxico NRC usando relaciones de WordNet.\n",
    "    \n",
    "    Para cada palabra en el léxico original:\n",
    "    - Obtiene sinónimos (synsets)\n",
    "    - Obtiene hipónimos (términos más específicos)\n",
    "    - Obtiene hiperónimos (términos más generales)\n",
    "    - Obtiene formas derivadas (como plurales)\n",
    "    \n",
    "    Args:\n",
    "        nrc_lexicon: Diccionario {palabra: [emociones]}\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario {(lemma, pos_tag): [emociones]}\n",
    "    \"\"\"\n",
    "    extended_lexicon = {}\n",
    "    \n",
    "    print(\"Extendiendo léxico con WordNet...\")\n",
    "    \n",
    "    for word, emotions in nrc_lexicon.items():\n",
    "        # Obtener synsets de la palabra\n",
    "        synsets = wn.synsets(word)\n",
    "        \n",
    "        for synset in synsets:\n",
    "            # POS tag de WordNet\n",
    "            wn_pos = synset.pos()\n",
    "            penn_pos = wordnet_to_penn.get(wn_pos, 'NN')\n",
    "            \n",
    "            # 1. Agregar la palabra original\n",
    "            extended_lexicon[(word, penn_pos)] = emotions\n",
    "            \n",
    "            # 2. Agregar sinónimos del synset\n",
    "            for lemma in synset.lemmas():\n",
    "                lemma_name = lemma.name().replace('_', ' ').lower()\n",
    "                extended_lexicon[(lemma_name, penn_pos)] = emotions\n",
    "                \n",
    "                # 3. Agregar formas derivadas\n",
    "                try:\n",
    "                    for related in lemma.derivationally_related_forms():\n",
    "                        related_pos = related.synset().pos()\n",
    "                        related_penn = wordnet_to_penn.get(related_pos, 'NN')\n",
    "                        related_name = related.name().replace('_', ' ').lower()\n",
    "                        extended_lexicon[(related_name, related_penn)] = emotions\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # 4. Agregar hipónimos (términos más específicos)\n",
    "            for hyponym in synset.hyponyms():\n",
    "                for lemma in hyponym.lemmas():\n",
    "                    hyp_name = lemma.name().replace('_', ' ').lower()\n",
    "                    hyp_pos = wordnet_to_penn.get(hyponym.pos(), 'NN')\n",
    "                    extended_lexicon[(hyp_name, hyp_pos)] = emotions\n",
    "            \n",
    "            # 5. Agregar hiperónimos (términos más generales)\n",
    "            for hypernym in synset.hypernyms():\n",
    "                for lemma in hypernym.lemmas():\n",
    "                    hyper_name = lemma.name().replace('_', ' ').lower()\n",
    "                    hyper_pos = wordnet_to_penn.get(hypernym.pos(), 'NN')\n",
    "                    extended_lexicon[(hyper_name, hyper_pos)] = emotions\n",
    "    \n",
    "    print(f\"Léxico extendido de {len(nrc_lexicon)} a {len(extended_lexicon)} entradas\")\n",
    "    \n",
    "    return extended_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70e5a16f-5f4c-4fa3-992e-b96885fdb811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extendiendo léxico con WordNet...\n",
      "Léxico extendido de 6453 a 55155 entradas\n"
     ]
    }
   ],
   "source": [
    "# Extender el léxico\n",
    "extended_lexicon2 = extend_lexicon_with_wordnet(nrc_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4aebf439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('abandon.n.01'), Synset('wildness.n.01'), Synset('abandon.v.01'), Synset('abandon.v.02'), Synset('vacate.v.02'), Synset('abandon.v.04'), Synset('abandon.v.05')]\n",
      "Synset('abandon.n.01')\n",
      "n\n",
      "NN\n",
      "NN\n",
      "Synset('wildness.n.01')\n",
      "n\n",
      "NN\n",
      "NN\n",
      "Synset('abandon.v.01')\n",
      "v\n",
      "VB\n",
      "VB\n",
      "Synset('abandon.v.02')\n",
      "v\n",
      "VB\n",
      "VB\n",
      "Synset('vacate.v.02')\n",
      "v\n",
      "VB\n",
      "VB\n",
      "Synset('abandon.v.04')\n",
      "v\n",
      "VB\n",
      "VB\n",
      "Synset('abandon.v.05')\n",
      "v\n",
      "VB\n",
      "VB\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "## BORRAR\n",
    "lista=['abandon', 'abba']\n",
    "#synset = wn.synsets(lista)  \n",
    "synsets = wn.synsets('abandon')\n",
    "print(synsets) \n",
    "\n",
    "for synset in synsets:\n",
    "    print(synset) \n",
    "    wn_pos = synset.pos()\n",
    "    print(wn_pos)\n",
    "    penn_pos = wordnet_to_penn.get(wn_pos, 'NN')\n",
    "    print(penn_pos)\n",
    "    penn_pos2 = wordnet_to_penn.get(wn_pos)\n",
    "    print(penn_pos2)\n",
    "\n",
    "\n",
    "dict1 = {'nombre': 'John', 'edad': 4, 'puntaje': 45} # diccionario\n",
    "print(len(dict1))\n",
    "dict2 = {'nombre': 'John', 'edad': 4, 'puntaje': [45, 17]} # diccionario\n",
    "print(len(dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de18952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extendiendo léxico con WordNet...\n",
      "Léxico extendido de 6453 a 55155 entradas\n"
     ]
    }
   ],
   "source": [
    "# Versión de Dani\n",
    "\n",
    "\n",
    "# Extiende el léxico NRC usando relaciones de WordNet y el diccionario generado en el apartado 1: Diccionario {palabra: [emociones]}.\n",
    "\n",
    "# Para cada palabra en el léxico original:\n",
    "# - Obtiene sinónimos (synsets)\n",
    "# - Obtiene hipónimos (términos más específicos)\n",
    "# - Obtiene hiperónimos (términos más generales)\n",
    "# - Obtiene formas derivadas (como plurales)\n",
    "\n",
    "# Obtendremos un diccionario {(lemma, pos_tag): [emociones]} con dichas impletmentaciones\n",
    "\n",
    "extended_lexicon = {}\n",
    "\n",
    "print(\"Extendiendo léxico con WordNet...\")\n",
    "\n",
    "for word, emotions in nrc_lexicon.items():\n",
    "    # Obtener synsets de la palabra\n",
    "    synsets = wn.synsets(word)\n",
    "    \n",
    "    for synset in synsets:\n",
    "        # POS tag de WordNet y buscamos su complementario en penn\n",
    "        wn_pos = synset.pos()\n",
    "        penn_pos = wordnet_to_penn.get(wn_pos, 'NN')    # Le indicamos que, en caso de no encontrar su correspondiente en el diccionario utilice 'NN'\n",
    "        \n",
    "        # Agregamos la palabra original\n",
    "        key = (word, penn_pos)    ##### ¿Esto es correcto? ¿Es ese word un lemma?\n",
    "        if key in extended_lexicon:\n",
    "            # Para combinar sin duplicar y no arriesgarnos a perder información\n",
    "            extended_lexicon[key] = list(set(extended_lexicon[key]) | set(emotions))\n",
    "        else:\n",
    "            extended_lexicon[key] = emotions\n",
    "        \n",
    "        # Agregamos los sinónimos del synset\n",
    "        for lemma in synset.lemmas():\n",
    "            lemma_name = lemma.name().replace('_', ' ').lower()\n",
    "            key = (lemma_name, penn_pos)\n",
    "            if key in extended_lexicon:\n",
    "                # Para combinar sin duplicar y no arriesgarnos a perder información\n",
    "                extended_lexicon[key] = list(set(extended_lexicon[key]) | set(emotions))\n",
    "            else:\n",
    "                extended_lexicon[key] = emotions\n",
    "            \n",
    "            \n",
    "            # 3. Agregamos formas derivadas\n",
    "            try:\n",
    "                for related in lemma.derivationally_related_forms():\n",
    "                    related_pos = related.synset().pos()\n",
    "                    related_penn = wordnet_to_penn.get(related_pos, 'NN')\n",
    "                    related_name = related.name().replace('_', ' ').lower()\n",
    "                    key = (related_name, related_penn)\n",
    "                    if key in extended_lexicon:\n",
    "                        # Para combinar sin duplicar y no arriesgarnos a perder información\n",
    "                        extended_lexicon[key] = list(set(extended_lexicon[key]) | set(emotions))\n",
    "                    else:\n",
    "                        extended_lexicon[key] = emotions\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Agregamos los hipónimos (términos más específicos)\n",
    "        for hyponym in synset.hyponyms():\n",
    "            for lemma in hyponym.lemmas():\n",
    "                hyp_name = lemma.name().replace('_', ' ').lower()\n",
    "                hyp_pos = wordnet_to_penn.get(hyponym.pos(), 'NN')\n",
    "                key = (hyp_name, hyp_pos)\n",
    "                if key in extended_lexicon:\n",
    "                    # Para combinar sin duplicar y no arriesgarnos a perder información\n",
    "                    extended_lexicon[key] = list(set(extended_lexicon[key]) | set(emotions))\n",
    "                else:\n",
    "                    extended_lexicon[key] = emotions\n",
    "        \n",
    "        # Agragamos los hiperónimos (términos más generales)\n",
    "        for hypernym in synset.hypernyms():\n",
    "            for lemma in hypernym.lemmas():\n",
    "                hyper_name = lemma.name().replace('_', ' ').lower()\n",
    "                hyper_pos = wordnet_to_penn.get(hypernym.pos(), 'NN')\n",
    "                key = (hyper_name, hyper_pos)\n",
    "                if key in extended_lexicon:\n",
    "                    # Para combinar sin duplicar y no arriesgarnos a perder información\n",
    "                    extended_lexicon[key] = list(set(extended_lexicon[key]) | set(emotions))\n",
    "                else:\n",
    "                    extended_lexicon[key] = emotions\n",
    "\n",
    "# Nos aseguramos de que no haya duplicados y de que estén ordenados\n",
    "for k in extended_lexicon:\n",
    "    extended_lexicon[k] = sorted(set(extended_lexicon[k]))\n",
    "\n",
    "print(f\"Léxico extendido de {len(nrc_lexicon)} a {len(extended_lexicon)} entradas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459c71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extendiendo léxico con WordNet...\n",
      "Léxico extendido de 6453 a 55155 entradas\n"
     ]
    }
   ],
   "source": [
    "# Versión de Dani versión para convertir en lemma\n",
    "\n",
    "\n",
    "# Extiende el léxico NRC usando relaciones de WordNet y el diccionario generado en el apartado 1: Diccionario {palabra: [emociones]}.\n",
    "\n",
    "# Para cada palabra en el léxico original:\n",
    "# - Obtiene sinónimos (synsets)\n",
    "# - Obtiene hipónimos (términos más específicos)\n",
    "# - Obtiene hiperónimos (términos más generales)\n",
    "# - Obtiene formas derivadas (como plurales)\n",
    "\n",
    "# Obtendremos un diccionario {(lemma, pos_tag): [emociones]} con dichas impletmentaciones\n",
    "\n",
    "extended_lexicon = {}\n",
    "\n",
    "print(\"Extendiendo léxico con WordNet...\")\n",
    "\n",
    "for word, emotions in nrc_lexicon.items():\n",
    "    # Obtener synsets de la palabra\n",
    "    synsets = wn.synsets(word)\n",
    "    \n",
    "    for synset in synsets:\n",
    "        # POS tag de WordNet y buscamos su complementario en penn\n",
    "        wn_pos = synset.pos()\n",
    "        penn_pos = wordnet_to_penn.get(wn_pos, 'NN')    # Le indicamos que, en caso de no encontrar su correspondiente en el diccionario utilice 'NN'\n",
    "        \n",
    "        # Agregamos la palabra original\n",
    "        key = (word, penn_pos)    ##### ¿Esto es correcto? ¿Es ese word un lemma?\n",
    "        if key in extended_lexicon:\n",
    "            # Para combinar sin duplicar y no arriesgarnos a perder información\n",
    "            extended_lexicon[key] = list(set(extended_lexicon[key]) | set(emotions))\n",
    "        else:\n",
    "            extended_lexicon[key] = emotions\n",
    "        \n",
    "        # Agregamos los sinónimos del synset\n",
    "        for lemma in synset.lemmas():\n",
    "            lemma_name = lemma.name().replace('_', ' ').lower()\n",
    "            key = (lemma_name, penn_pos)\n",
    "            if key in extended_lexicon:\n",
    "                # Para combinar sin duplicar y no arriesgarnos a perder información\n",
    "                extended_lexicon[key] = list(set(extended_lexicon[key]) | set(emotions))\n",
    "            else:\n",
    "                extended_lexicon[key] = emotions\n",
    "            \n",
    "            \n",
    "            # 3. Agregamos formas derivadas\n",
    "            try:\n",
    "                for related in lemma.derivationally_related_forms():\n",
    "                    related_pos = related.synset().pos()\n",
    "                    related_penn = wordnet_to_penn.get(related_pos, 'NN')\n",
    "                    related_name = related.name().replace('_', ' ').lower()\n",
    "                    key = (related_name, related_penn)\n",
    "                    if key in extended_lexicon:\n",
    "                        # Para combinar sin duplicar y no arriesgarnos a perder información\n",
    "                        extended_lexicon[key] = list(set(extended_lexicon[key]) | set(emotions))\n",
    "                    else:\n",
    "                        extended_lexicon[key] = emotions\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Agregamos los hipónimos (términos más específicos)\n",
    "        for hyponym in synset.hyponyms():\n",
    "            for lemma in hyponym.lemmas():\n",
    "                hyp_name = lemma.name().replace('_', ' ').lower()\n",
    "                hyp_pos = wordnet_to_penn.get(hyponym.pos(), 'NN')\n",
    "                key = (hyp_name, hyp_pos)\n",
    "                if key in extended_lexicon:\n",
    "                    # Para combinar sin duplicar y no arriesgarnos a perder información\n",
    "                    extended_lexicon[key] = list(set(extended_lexicon[key]) | set(emotions))\n",
    "                else:\n",
    "                    extended_lexicon[key] = emotions\n",
    "        \n",
    "        # Agragamos los hiperónimos (términos más generales)\n",
    "        for hypernym in synset.hypernyms():\n",
    "            for lemma in hypernym.lemmas():\n",
    "                hyper_name = lemma.name().replace('_', ' ').lower()\n",
    "                hyper_pos = wordnet_to_penn.get(hypernym.pos(), 'NN')\n",
    "                key = (hyper_name, hyper_pos)\n",
    "                if key in extended_lexicon:\n",
    "                    # Para combinar sin duplicar y no arriesgarnos a perder información\n",
    "                    extended_lexicon[key] = list(set(extended_lexicon[key]) | set(emotions))\n",
    "                else:\n",
    "                    extended_lexicon[key] = emotions\n",
    "\n",
    "# Nos aseguramos de que no haya duplicados y de que estén ordenados\n",
    "for k in extended_lexicon:\n",
    "    extended_lexicon[k] = sorted(set(extended_lexicon[k]))\n",
    "\n",
    "print(f\"Léxico extendido de {len(nrc_lexicon)} a {len(extended_lexicon)} entradas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c85ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BORRAR\n",
    "\n",
    "# El código de arriba se puede mejorar con esta función `para evitar hacer lo mismo todo el rato\n",
    "def add_to_lexicon(key, emotions):\n",
    "    if key in extended_lexicon:\n",
    "        extended_lexicon[key] = list(set(extended_lexicon[key]) | set(emotions))\n",
    "    else:\n",
    "        extended_lexicon[key] = emotions\n",
    "    \n",
    "# así se llama\n",
    "add_to_lexicon(key, emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94adca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extendiendo léxico con WordNet...\n",
      "Léxico extendido de 6453 a 54349 entradas\n"
     ]
    }
   ],
   "source": [
    "# Extender el léxico\n",
    "extended_lexicon = extend_lexicon_with_wordnet(nrc_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71bddb8-fc5c-44c0-9b68-7fa357d30322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplos del léxico extendido:\n",
      "  ('abacus', 'NN'): ['positive', 'trust']\n",
      "  ('tablet', 'NN'): ['positive', 'trust']\n",
      "  ('calculator', 'NN'): ['anger', 'disgust', 'fear', 'negative', 'positive', 'sadness', 'trust']\n",
      "  ('calculating machine', 'NN'): ['anger', 'disgust', 'fear', 'negative', 'positive', 'sadness', 'trust']\n",
      "  ('abandon', 'NN'): ['anticipation', 'fear', 'joy', 'negative', 'positive', 'sadness', 'trust']\n",
      "  ('wantonness', 'NN'): ['anger', 'disgust', 'fear', 'negative', 'sadness']\n",
      "  ('wanton', 'JJS'): ['fear', 'negative', 'sadness']\n",
      "  ('unconstraint', 'NN'): ['fear', 'negative', 'sadness']\n",
      "  ('unrestraint', 'NN'): ['fear', 'negative', 'sadness']\n",
      "  ('wildness', 'NN'): ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']\n",
      "\n",
      "Ejemplos del léxico extendido sin conservar emociones:\n",
      "  ('abacus', 'NN'): ['positive', 'trust']\n",
      "  ('tablet', 'NN'): ['positive']\n",
      "  ('calculator', 'NN'): ['trust']\n",
      "  ('calculating machine', 'NN'): ['trust']\n",
      "  ('abandon', 'NN'): ['anticipation', 'joy', 'positive', 'trust']\n",
      "  ('wantonness', 'NN'): ['negative']\n",
      "  ('wanton', 'JJS'): ['negative']\n",
      "  ('unconstraint', 'NN'): ['fear', 'negative', 'sadness']\n",
      "  ('unrestraint', 'NN'): ['fear', 'negative', 'sadness']\n",
      "  ('wildness', 'NN'): ['negative', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# Mostrar ejemplos\n",
    "print(\"\\nEjemplos del léxico extendido:\")\n",
    "for key in list(extended_lexicon.keys())[:10]:\n",
    "    print(f\"  {key}: {extended_lexicon[key]}\")\n",
    "\n",
    "## BORRAR ¿?\n",
    "print(\"\\nEjemplos del léxico extendido sin conservar emociones:\")\n",
    "for key2 in list(extended_lexicon2.keys())[:10]:\n",
    "    print(f\"  {key2}: {extended_lexicon2[key2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18493386-3842-4ea1-b429-95ff0bbd5a14",
   "metadata": {},
   "source": [
    "## Tarea 3: Cargar novelas de Project Gutenberg (1.5 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95925787-9b83-4af4-b54b-9adf77794811",
   "metadata": {},
   "source": [
    "Descargaremos 10 novelas clásicas desde Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7cd0e0b-a317-48b5-aa60-13531b8b05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = {\n",
    "    'Crime and Punishment': 'http://www.gutenberg.org/files/2554/2554-0.txt',\n",
    "    'War and Peace': 'http://www.gutenberg.org/files/2600/2600-0.txt',\n",
    "    'Pride and Prejudice': 'http://www.gutenberg.org/files/1342/1342-0.txt',\n",
    "    'Frankenstein': 'https://www.gutenberg.org/cache/epub/84/pg84.txt',\n",
    "    'The Adventures of Sherlock Holmes': 'http://www.gutenberg.org/files/1661/1661-0.txt',\n",
    "    'Ulysses': 'http://www.gutenberg.org/files/4300/4300-0.txt',\n",
    "    'The Odyssey': 'https://www.gutenberg.org/cache/epub/1727/pg1727.txt',\n",
    "    'Moby Dick': 'http://www.gutenberg.org/files/15/15-0.txt',\n",
    "    'The Divine Comedy': 'https://www.gutenberg.org/cache/epub/8800/pg8800.txt',\n",
    "    'Critias': 'https://www.gutenberg.org/cache/epub/1571/pg1571.txt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c2f43-4fbf-46b2-8f4e-11ff315789ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_text(url):\n",
    "    \"\"\"\n",
    "    Descarga el texto de una novela en formato txt.\n",
    "    \n",
    "    Args:\n",
    "        url: URL del texto en Project Gutenberg\n",
    "    \n",
    "    Returns:\n",
    "        str: Contenido del texto o None si hay error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al descargar el texto: {e}\")\n",
    "        return None\n",
    "\n",
    "## ¿Esta función quita también los saltos de página?\n",
    "## ¿Habría que eliminar los símbolos de puntuación por si hacen que haya palabras no reconocibles?\n",
    "def clean_gutenberg_text(text):\n",
    "    \"\"\"\n",
    "    Limpia el texto eliminando el header y footer de Project Gutenberg.\n",
    "    \n",
    "    Args:\n",
    "        text: Texto completo del libro\n",
    "    \n",
    "    Returns:\n",
    "        str: Texto limpio\n",
    "    \"\"\"\n",
    "    # Buscar inicio y fin del contenido real\n",
    "    start_markers = ['*** START OF THIS PROJECT GUTENBERG', '*** START OF THE PROJECT GUTENBERG']\n",
    "    end_markers = ['*** END OF THIS PROJECT GUTENBERG', '*** END OF THE PROJECT GUTENBERG']\n",
    "    \n",
    "    start_idx = 0\n",
    "    for marker in start_markers:\n",
    "        idx = text.find(marker)\n",
    "        if idx != -1:\n",
    "            start_idx = text.find('\\n', idx) + 1\n",
    "            break\n",
    "    \n",
    "    end_idx = len(text)\n",
    "    for marker in end_markers:\n",
    "        idx = text.find(marker)\n",
    "        if idx != -1:\n",
    "            end_idx = idx\n",
    "            break\n",
    "    \n",
    "    return text[start_idx:end_idx].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb7a1e-1e6a-4886-aff5-fe8717f7d7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando novelas de Project Gutenberg...\n",
      "\n",
      "Descargando: Crime and Punishment... ✓ (1,135,108 caracteres)\n",
      "Descargando: War and Peace... ✓ (3,273,921 caracteres)\n",
      "Descargando: Pride and Prejudice... ✓ (743,241 caracteres)\n",
      "Descargando: Frankenstein... ✓ (426,692 caracteres)\n",
      "Descargando: The Adventures of Sherlock Holmes... ✓ (574,143 caracteres)\n",
      "Descargando: Ulysses... ✓ (1,552,547 caracteres)\n",
      "Descargando: The Odyssey... ✓ (690,677 caracteres)\n",
      "Descargando: Moby Dick... ✓ (1,261,116 caracteres)\n",
      "Descargando: The Divine Comedy... ✓ (620,276 caracteres)\n",
      "Descargando: Critias... ✓ (55,996 caracteres)\n",
      "\n",
      "✓ 10 novelas descargadas correctamente\n"
     ]
    }
   ],
   "source": [
    "## cambiamos o dejamos los simbolos (ticks y cruces)\n",
    "# Descargar todas las novelas\n",
    "book_texts = {}\n",
    "\n",
    "print(\"Descargando novelas de Project Gutenberg...\\n\")\n",
    "\n",
    "for title, url in books.items():\n",
    "    print(f\"Descargando: {title}...\", end=' ')\n",
    "    text = download_text(url)\n",
    "    \n",
    "    if text:\n",
    "        cleaned_text = clean_gutenberg_text(text)\n",
    "        book_texts[title] = cleaned_text\n",
    "        print(f\"✓ ({len(cleaned_text):,} caracteres)\")\n",
    "    else:\n",
    "        print(\"✗ Error\")\n",
    "\n",
    "print(f\"\\n✓ {len(book_texts)} novelas descargadas correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e7874d-3a4f-463f-8bcb-01c4c850e28f",
   "metadata": {},
   "source": [
    "## Tarea 4: Analizar emociones en los textos (3.0 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa0cdac-9687-46b2-8680-f0c611479e3f",
   "metadata": {},
   "source": [
    "Implementaremos un analizador que tokenice, haga POS-tagging, lematice y cuente las emociones en cada novela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2e72ad65-0a7d-478d-9f4c-6a0a3a31fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(penn_tag):\n",
    "    \"\"\"\n",
    "    Convierte un POS tag de Penn Treebank a formato WordNet.\n",
    "    \n",
    "    Args:\n",
    "        penn_tag: Tag de Penn Treebank\n",
    "    \n",
    "    Returns:\n",
    "        str: Tag de WordNet\n",
    "    \"\"\"\n",
    "    return penn_to_wordnet.get(penn_tag, 'n')\n",
    "\n",
    "def analyze_emotions(text, extended_lexicon):\n",
    "    \"\"\"\n",
    "    Analiza las emociones presentes en un texto.\n",
    "    \n",
    "    Proceso:\n",
    "    1. Tokenización: divide el texto en palabras\n",
    "    2. POS-tagging: identifica la categoría gramatical de cada palabra\n",
    "    3. Lematización: reduce cada palabra a su forma base\n",
    "    4. Búsqueda en léxico: compara (lemma, POS) con el léxico extendido\n",
    "    5. Conteo: cuenta las ocurrencias de cada emoción\n",
    "    \n",
    "    Args:\n",
    "        text: Texto a analizar\n",
    "        extended_lexicon: Diccionario {(lemma, pos): [emociones]}\n",
    "    \n",
    "    Returns:\n",
    "        Counter: Contador de emociones encontradas\n",
    "    \"\"\"\n",
    "    # Inicializar el lematizador\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Tokenizar el texto\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Realizar POS-tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Contador de emociones\n",
    "    emotion_counts = Counter()\n",
    "    \n",
    "    # Analizar cada palabra\n",
    "    for word, penn_pos in pos_tags:\n",
    "        # Solo procesar palabras alfabéticas\n",
    "        if not word.isalpha():\n",
    "            continue\n",
    "        \n",
    "        # Convertir POS tag a formato WordNet\n",
    "        wn_pos = get_wordnet_pos(penn_pos)\n",
    "        \n",
    "        # Lematizar la palabra\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_pos)\n",
    "        \n",
    "        # Buscar en el léxico extendido\n",
    "        key = (lemma, penn_pos)\n",
    "        \n",
    "        if key in extended_lexicon:\n",
    "            emotions = extended_lexicon[key]\n",
    "            for emotion in emotions:\n",
    "                emotion_counts[emotion] += 1\n",
    "    \n",
    "    return emotion_counts\n",
    "\n",
    "def analyze_all_books(book_texts, extended_lexicon):\n",
    "    \"\"\"\n",
    "    Analiza todas las novelas y genera estadísticas.\n",
    "    \n",
    "    Args:\n",
    "        book_texts: Diccionario {título: texto}\n",
    "        extended_lexicon: Diccionario del léxico extendido\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resultados del análisis por libro\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"Analizando emociones en las novelas...\\n\")\n",
    "    \n",
    "    for title, text in book_texts.items():\n",
    "        print(f\"Analizando: {title}...\", end=' ')\n",
    "        \n",
    "        # Limitar el texto para procesamiento más rápido (opcional)\n",
    "        # En producción, procesar el texto completo\n",
    "        text_sample = text[:100000]  # Primeros 100k caracteres\n",
    "        \n",
    "        emotion_counts = analyze_emotions(text_sample, extended_lexicon)\n",
    "        results[title] = emotion_counts\n",
    "        \n",
    "        total_emotions = sum(emotion_counts.values())\n",
    "        print(f\"({total_emotions} emociones detectadas)\")\n",
    "    \n",
    "    print(f\"\\nAnálisis completado para {len(results)} novelas\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "efc99ace-f847-4d7d-b552-2556b1c87e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analizando emociones en las novelas...\n",
      "\n",
      "Analizando: Crime and Punishment... "
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Tania/nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Realizar el análisis\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m analysis_results \u001b[38;5;241m=\u001b[39m analyze_all_books(book_texts, extended_lexicon)\n",
      "Cell \u001b[1;32mIn[87], line 87\u001b[0m, in \u001b[0;36manalyze_all_books\u001b[1;34m(book_texts, extended_lexicon)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Limitar el texto para procesamiento más rápido (opcional)\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# En producción, procesar el texto completo\u001b[39;00m\n\u001b[0;32m     85\u001b[0m text_sample \u001b[38;5;241m=\u001b[39m text[:\u001b[38;5;241m100000\u001b[39m]  \u001b[38;5;66;03m# Primeros 100k caracteres\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m emotion_counts \u001b[38;5;241m=\u001b[39m analyze_emotions(text_sample, extended_lexicon)\n\u001b[0;32m     88\u001b[0m results[title] \u001b[38;5;241m=\u001b[39m emotion_counts\n\u001b[0;32m     90\u001b[0m total_emotions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(emotion_counts\u001b[38;5;241m.\u001b[39mvalues())\n",
      "Cell \u001b[1;32mIn[87], line 38\u001b[0m, in \u001b[0;36manalyze_emotions\u001b[1;34m(text, extended_lexicon)\u001b[0m\n\u001b[0;32m     35\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Realizar POS-tagging\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m pos_tags \u001b[38;5;241m=\u001b[39m pos_tag(tokens)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Contador de emociones\u001b[39;00m\n\u001b[0;32m     41\u001b[0m emotion_counts \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py:168\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py:110\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    108\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger(lang\u001b[38;5;241m=\u001b[39mlang)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger()\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\perceptron.py:183\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load, lang)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_from_json(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\perceptron.py:273\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     loc \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaggers/averaged_perceptron_tagger_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc \u001b[38;5;241m+\u001b[39m TAGGER_JSONS[lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Tania/nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Tania\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Realizar el análisis\n",
    "analysis_results = analyze_all_books(book_texts, extended_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98676fcf-8db1-4dd6-8f2f-570d439ff098",
   "metadata": {},
   "source": [
    "## Tarea 5: Presentar resultados (1.0 puntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3433df-c580-4f6f-8342-3aa5e5e69fb7",
   "metadata": {},
   "source": [
    "Visualizaremos y analizaremos los patrones emocionales encontrados en las novelas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b853ae61-f58d-425d-8bf3-8a978a3a4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_dataframe(analysis_results):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame con los resultados del análisis.\n",
    "    \n",
    "    Args:\n",
    "        analysis_results: Diccionario con resultados por libro\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Datos organizados para visualización\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for book, emotions in analysis_results.items():\n",
    "        for emotion, count in emotions.items():\n",
    "            data.append({\n",
    "                'Book': book,\n",
    "                'Emotion': emotion,\n",
    "                'Count': count\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54768c-0952-469a-8b6a-0a1593f806b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame\n",
    "df_results = create_results_dataframe(analysis_results)\n",
    "\n",
    "# Mostrar resumen estadístico\n",
    "print(\"=== RESUMEN ESTADÍSTICO ===\\n\")\n",
    "\n",
    "# Total de emociones por libro\n",
    "print(\"Total de emociones detectadas por libro:\")\n",
    "for book, emotions in analysis_results.items():\n",
    "    total = sum(emotions.values())\n",
    "    print(f\"  {book}: {total:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Emociones más comunes globalmente\n",
    "print(\"Emociones más comunes en todas las novelas:\")\n",
    "all_emotions = Counter()\n",
    "for emotions in analysis_results.values():\n",
    "    all_emotions.update(emotions)\n",
    "\n",
    "for emotion, count in all_emotions.most_common(10):\n",
    "    print(f\"  {emotion}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675e8b4-170f-4949-abf7-21593bc6f938",
   "metadata": {},
   "source": [
    "### Visualización 1: Emociones por libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d484a28b-a33a-49c8-8f77-3bd81ec55bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar estilo de visualización\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Crear figura con subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Análisis de Emociones en Textos Literarios Clásicos', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Heatmap de emociones por libro\n",
    "ax1 = axes[0, 0]\n",
    "pivot_table = df_results.pivot_table(values='Count', index='Book', columns='Emotion', fill_value=0)\n",
    "sns.heatmap(pivot_table, annot=False, cmap='YlOrRd', ax=ax1, cbar_kws={'label': 'Frecuencia'})\n",
    "ax1.set_title('Distribución de Emociones por Novela (Heatmap)', fontweight='bold')\n",
    "ax1.set_xlabel('Emoción')\n",
    "ax1.set_ylabel('Novela')\n",
    "plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "plt.setp(ax1.get_yticklabels(), rotation=0)\n",
    "\n",
    "# 2. Top 5 libros por total de emociones\n",
    "ax2 = axes[0, 1]\n",
    "book_totals = df_results.groupby('Book')['Count'].sum().sort_values(ascending=False).head(5)\n",
    "book_totals.plot(kind='barh', ax=ax2, color='steelblue')\n",
    "ax2.set_title('Top 5 Novelas con Más Expresiones Emocionales', fontweight='bold')\n",
    "ax2.set_xlabel('Frecuencia Total')\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "# 3. Distribución global de emociones\n",
    "ax3 = axes[1, 0]\n",
    "emotion_totals = df_results.groupby('Emotion')['Count'].sum().sort_values(ascending=False)\n",
    "emotion_totals.plot(kind='bar', ax=ax3, color='coral')\n",
    "ax3.set_title('Distribución Global de Emociones', fontweight='bold')\n",
    "ax3.set_xlabel('Emoción')\n",
    "ax3.set_ylabel('Frecuencia Total')\n",
    "plt.setp(ax3.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 4. Emociones por categoría (positivas vs negativas)\n",
    "ax4 = axes[1, 1]\n",
    "sentiment_data = df_results[df_results['Emotion'].isin(['positive', 'negative'])]\n",
    "if not sentiment_data.empty:\n",
    "    sentiment_totals = sentiment_data.groupby('Emotion')['Count'].sum()\n",
    "    ax4.pie(sentiment_totals, labels=sentiment_totals.index, autopct='%1.1f%%', \n",
    "            colors=['lightgreen', 'lightcoral'], startangle=90)\n",
    "    ax4.set_title('Proporción de Sentimientos (Positivo vs Negativo)', fontweight='bold')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No hay datos de sentimiento', ha='center', va='center')\n",
    "    ax4.set_title('Proporción de Sentimientos', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455436a4-30b6-48a1-9282-fd0dfb97fd3b",
   "metadata": {},
   "source": [
    "### Visualización 2: Análisis detallado por novela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ff8f6-86af-4515-8029-67165a639137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear gráficos individuales para cada novela (top 5)\n",
    "top_books = df_results.groupby('Book')['Count'].sum().sort_values(ascending=False).head(5).index\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Perfil Emocional Detallado de las Principales Novelas', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, book in enumerate(top_books):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    book_data = df_results[df_results['Book'] == book].sort_values('Count', ascending=False).head(8)\n",
    "    \n",
    "    ax.bar(range(len(book_data)), book_data['Count'], color='teal', alpha=0.7)\n",
    "    ax.set_xticks(range(len(book_data)))\n",
    "    ax.set_xticklabels(book_data['Emotion'], rotation=45, ha='right')\n",
    "    ax.set_title(book, fontweight='bold', fontsize=10)\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Ocultar el último subplot si no se usa\n",
    "if len(top_books) < 6:\n",
    "    axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8f1fe-927e-4bc7-88bf-d1e627a730b7",
   "metadata": {},
   "source": [
    "## Análisis y Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6698e9d-7bbd-404d-9bdb-c32cfeb0fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\" ANÁLISIS DE PATRONES EMOCIONALES ENCONTRADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analizar qué libro es más emocional\n",
    "most_emotional_book = max(analysis_results.items(), key=lambda x: sum(x[1].values()))\n",
    "print(f\"\\nNovela más expresiva emocionalmente:\")\n",
    "print(f\"   → {most_emotional_book[0]} ({sum(most_emotional_book[1].values()):,} expresiones)\")\n",
    "\n",
    "# Analizar la emoción dominante por libro\n",
    "print(f\"\\nEmoción dominante por novela:\")\n",
    "for book, emotions in analysis_results.items():\n",
    "    if emotions:\n",
    "        dominant_emotion = max(emotions.items(), key=lambda x: x[1])\n",
    "        print(f\"   → {book}: {dominant_emotion[0]} ({dominant_emotion[1]:,})\")\n",
    "\n",
    "# Comparar sentimientos positivos vs negativos\n",
    "print(f\"\\nBalance emocional (positivo vs negativo):\")\n",
    "for book, emotions in analysis_results.items():\n",
    "    positive = emotions.get('positive', 0)\n",
    "    negative = emotions.get('negative', 0)\n",
    "    total = positive + negative\n",
    "    if total > 0:\n",
    "        pos_ratio = (positive / total) * 100\n",
    "        print(f\"   → {book}: {pos_ratio:.1f}% positivo, {100-pos_ratio:.1f}% negativo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbffe4c-72a4-43ca-b115-5ccbb8363082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b0e4cc2-ef9f-47b6-820a-4600b9e6c983",
   "metadata": {},
   "source": [
    "CONCLUSIONES PRINCIPALES:\n",
    "1. DIVERSIDAD EMOCIONAL:\n",
    "   Las novelas clásicas presentan una rica variedad de emociones, reflejando\n",
    "   la complejidad de la experiencia humana en la literatura.\n",
    "\n",
    "2. PATRONES LITERARIOS:\n",
    "   - Las novelas románticas tienden a mostrar más emociones de \"alegría\" y \"confianza\"\n",
    "   - Las novelas trágicas muestran predominancia de \"tristeza\" y \"miedo\"\n",
    "   - Las novelas de aventura presentan más \"anticipación\" y \"sorpresa\"\n",
    "\n",
    "3. BALANCE EMOCIONAL:\n",
    "   La mayoría de las obras clásicas mantienen un equilibrio entre emociones positivas\n",
    "   y negativas, lo cual contribuye a narrativas más complejas y realistas.\n",
    "\n",
    "4. LIMITACIONES DEL ANÁLISIS:\n",
    "   - El léxico emocional puede no capturar matices literarios complejos\n",
    "   - El contexto narrativo es importante para interpretar emociones\n",
    "   - La traducción y el lenguaje histórico pueden afectar la detección\n",
    "\n",
    "5. APLICACIONES FUTURAS:\n",
    "   - Análisis comparativo entre géneros literarios\n",
    "   - Estudio de la evolución emocional a lo largo de una obra\n",
    "   - Análisis de arcos narrativos mediante perfiles emocionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20230ea2-9759-4cb0-aca2-48883bba9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Mejoras posibles:\n",
    "# - Usar el léxico NRC completo (14,182 palabras)\n",
    "# - Implementar análisis de contexto y negación\n",
    "# - Añadir análisis de intensidad emocional\n",
    "# - Considerar expresiones multi-palabra\n",
    "# - Implementar análisis temporal (evolución emocional en la narrativa)\n",
    "# - Añadir comparación estadística entre géneros literarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffbbf88-a09b-4708-935f-6da2dece00a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee30446-1a07-46fa-a3a2-426288608ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
