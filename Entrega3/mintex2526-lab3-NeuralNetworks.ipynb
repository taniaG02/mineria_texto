{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afuUK4pmjPcN"
   },
   "source": [
    "**Master en Bioinformática y Biología Computacional, UAM**\n",
    "## **Minería de texto**\n",
    "# **Práctica de laboratorio 3: Redes neuronales para análisis de sentimientos**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPcld7lbmFp5"
   },
   "source": [
    "# ***Ejercicio 1 - FFNs***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFnOBQJ3jb82"
   },
   "source": [
    "# Carga y preprocesado de datos - One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "xn1SAWpkaJmK",
    "outputId": "b254edf0-ca8e-4230-ba7e-9e1d8e66f9f7"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Montamos el drive\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Montamos el drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02tQ_zO5eYYO"
   },
   "outputs": [],
   "source": [
    "# Fijamos semillas para generación de números aleatorios\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "-a4Yo6q9eyrx",
    "outputId": "2dd598d3-e6a1-4d1f-bf55-e91b6217c895"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'drive/My Drive/Colab Notebooks/mintex2526-lab3/data/sst_training.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m     23\u001b[0m data_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrive/My Drive/Colab Notebooks/mintex2526-lab3/data/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 24\u001b[0m training_set \u001b[38;5;241m=\u001b[39m load_sst_dataset(data_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msst_training.txt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# conjunto de datos de entrenamiento\u001b[39;00m\n\u001b[0;32m     25\u001b[0m validation_set \u001b[38;5;241m=\u001b[39m load_sst_dataset(data_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msst_validation.txt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# conjunto de datos de validación\u001b[39;00m\n\u001b[0;32m     26\u001b[0m test_set \u001b[38;5;241m=\u001b[39m load_sst_dataset(data_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msst_test.txt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# conjunto de datos de test\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m, in \u001b[0;36mload_sst_dataset\u001b[1;34m(file_path, label_map)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_sst_dataset\u001b[39m(file_path, label_map\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m1\u001b[39m}):\n\u001b[0;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(f):\n\u001b[0;32m     11\u001b[0m             instance \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive/Colab Notebooks/mintex2526-lab3/data/sst_training.txt'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Utilizamos el Stanford Sentiment Treebank (SST) como corpus de prueba para análisis de sentimientos de textos\n",
    "# Cargamos y procesamos los datos para tratarlos como un problema de clasificación binaria de sentimientos\n",
    "# Convertimos la escala de sentimientos de [1,5] a {0,1}\n",
    "def load_sst_dataset(file_path, label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n",
    "    data = []\n",
    "    with open(file_path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            instance = {}\n",
    "            instance['label'] = label_map[int(line[1])]\n",
    "            if instance['label'] is None:\n",
    "                continue\n",
    "\n",
    "            # Filtramos caracteres y etiquetas de parseo\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            instance['text'] = text[1:]\n",
    "            data.append(instance)\n",
    "    data = pd.DataFrame(data)\n",
    "    return data\n",
    "\n",
    "data_folder = 'drive/My Drive/Colab Notebooks/mintex2526-lab3/data/'\n",
    "training_set = load_sst_dataset(data_folder + 'sst_training.txt') # conjunto de datos de entrenamiento\n",
    "validation_set = load_sst_dataset(data_folder + 'sst_validation.txt') # conjunto de datos de validación\n",
    "test_set = load_sst_dataset(data_folder + 'sst_test.txt') # conjunto de datos de test\n",
    "\n",
    "print('Instancias de entrenamiento: {}'.format(len(training_set)))\n",
    "print('Instancias de de validación: {}'.format(len(validation_set)))\n",
    "print('Instancias de de test: {}'.format(len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "UNqXP_FqjL69",
    "outputId": "efe8b643-35ac-4213-fd86-9637555cb25c"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import text\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Desordenamos los conjuntos de datos\n",
    "training_set = shuffle(training_set)\n",
    "validation_set = shuffle(validation_set)\n",
    "test_set = shuffle(test_set)\n",
    "\n",
    "# Separamos textos y etiquetas de los conjuntos de datos\n",
    "training_texts = training_set.text\n",
    "training_labels = training_set.label\n",
    "validation_texts = validation_set.text\n",
    "validation_labels = validation_set.label\n",
    "test_texts = test_set.text\n",
    "test_labels = test_set.label\n",
    "\n",
    "# Construimos un índice (vocabulario) para las 1000 palabras más frecuentes en el conjunto de datos de entrenamiento\n",
    "tokenizer = text.Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(training_texts)\n",
    "\n",
    "print(\"10 palabras del vocabulario con mayor frecuencia:\")\n",
    "words_index = tokenizer.word_index\n",
    "for word, index in list(words_index.items())[:10]:\n",
    "    count = tokenizer.word_counts[word]\n",
    "    print(f\"\\t{word}  (índice: {index}, frecuencia: {count})\")\n",
    "print()\n",
    "\n",
    "print(\"10 palabras del vocabulario con menor frecuencia:\")\n",
    "sorted_word_counts = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=False)\n",
    "last_words_counts = sorted_word_counts[:10]\n",
    "for word, count in last_words_counts:\n",
    "    index = tokenizer.word_index[word]\n",
    "    print(f\"\\t{word}  (índice: {index}, frecuencia: {count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "kG59tQJkep1S",
    "outputId": "855542e2-f7a3-40f7-b941-a96f016df257"
   },
   "outputs": [],
   "source": [
    "# Vectorizamos los textos mediante la representación one-hot encoding\n",
    "x_training = tokenizer.texts_to_matrix(training_texts, mode='binary')\n",
    "y_training = training_labels\n",
    "x_validation = tokenizer.texts_to_matrix(validation_texts, mode='binary')\n",
    "y_validation = validation_labels\n",
    "x_test = tokenizer.texts_to_matrix(test_texts, mode='binary')\n",
    "y_test = test_labels\n",
    "\n",
    "print('Dimensiones del conjunto de entrenamiento: {}'.format(x_training.shape))\n",
    "print('Dimensiones del conjunto de validación: {}'.format(x_validation.shape))\n",
    "print('Dimensiones del conjunto de test: {}'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "euKctFAoNyN6",
    "outputId": "fe71083d-2c05-482d-cf05-c438be3a2a6b"
   },
   "outputs": [],
   "source": [
    "# Comprobación de algunos vectores generados:\n",
    "# Iteramos sobre n textos (de entrenamiento) y sus vectores correspondientes\n",
    "n = 3\n",
    "\n",
    "index_to_word = tokenizer.index_word # Mapeos de índices a palabras en el vocabulario\n",
    "word_counts = tokenizer.word_counts # Frecuencias de las palabras en los textos\n",
    "\n",
    "for i, (text, text_vector) in enumerate(zip(training_texts[:n], x_training[:n]), start=1):\n",
    "    print(f\"Texto {i}: {text}\")\n",
    "    print(f\"Vector: {text_vector}\")\n",
    "    print(\"Palabras:\")\n",
    "    for index, value in enumerate(text_vector):\n",
    "        # Si el valor en el vector es 1, obtenemos la palabra correspondiente, junto con su índice y frecuencia en el vocabulario\n",
    "        if value == 1:\n",
    "            word = index_to_word.get(index)\n",
    "            word_frequency = word_counts.get(word, 0) if word else 0\n",
    "            print(f\"\\t{word} (índice={index}, frecuencia={word_frequency})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7THuAMrBmGIM"
   },
   "source": [
    "# Construcción de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "GQ7h0p--ngQ-",
    "outputId": "bc836664-8958-4bb7-b207-c2ea42f207cd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "input_size = x_training[0].shape[0] # la longitud del vector de entrada es igual al tamaño del vocabulario\n",
    "\n",
    "# Definimos una red neuronal feedforward con una única capa oculta de 16 unidades\n",
    "# y ReLU como función de activación\n",
    "model = Sequential()\n",
    "model.add(Dense(units=16, activation='relu', input_shape=(input_size,))) # Nota: no es necesario indicar input_shape en capas sucesivas\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compilamos la red usando la entropía cruzada binaria como función de pérdida,\n",
    "# el algoritmo Adam como optimizador del descenso por gradiente,\n",
    "# y accuracy como métrica de evaluación en entrenamiento y validación\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Opcionalmente se pueden utilizar otras funciones de activación (e.g., tanh, sigmoid) en vez de ReLU\n",
    "# y otras funciones de pérdida como mean squared error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "5MSDgLZSp_X6",
    "outputId": "9907a7a8-ce29-461a-e9f2-5de141e72059"
   },
   "outputs": [],
   "source": [
    "# Entrenamos la red durante 100 épocas y con batches de tamaño 32\n",
    "history = model.fit(x_training, y_training, epochs=100, batch_size=32, validation_data=(x_validation, y_validation), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNlnFqscvnsS"
   },
   "source": [
    "**Ejercicio 1a - Red de una capa oculta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "4VgvwyYdqzgX",
    "outputId": "c5ac677b-b63c-46a3-dfb6-d0246abaebe2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualizamos un gráfica con el LOSS alcanzado en cada época para los conjuntos de entrenamiento y de validación\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('FNN 1 capa')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('época')\n",
    "plt.legend(['entrenamiento', 'validación'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Visualizamos un gráfica con el ACCURACY alcanzado en cada época para los conjuntos de entrenamiento y de validación\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeMOIOPZv6wc"
   },
   "source": [
    "**Ejercicio 1b - Red multicapa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4182
    },
    "id": "0nOnp5OEurxg",
    "outputId": "7326f05a-aed5-4de3-a94b-8102534464f9"
   },
   "outputs": [],
   "source": [
    "# Añadimos una capa oculta adicional al modelo\n",
    "model = Sequential()\n",
    "# TO DO\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history_2 = model.fit(x_training, y_training, epochs=100, batch_size=32, validation_data=(x_validation, y_validation), verbose=1)\n",
    "\n",
    "# Visualizamos una gráfica con la evolución de LOSS alcanzado en cada época para los conjuntos de entrenamiento y de validación\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "YauPhvCulR8Y",
    "outputId": "0071f159-f57b-4271-9681-23338f989659"
   },
   "outputs": [],
   "source": [
    "# Visualizamos una gráfica comparativa de la evolución de LOSS por épocas usando una o dos capas ocultas\n",
    "plt.title('FNN 1 capa vs. 2 capas')\n",
    "# TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQ_pNADpxz17"
   },
   "source": [
    "**Ejercicio 1c - Evaluación de hiperparámetros mediante grid search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "W_g6ztDMx-mz",
    "outputId": "cd99aa28-8954-416b-a43e-c6a5c8b6f6bf"
   },
   "outputs": [],
   "source": [
    "# Comentario: para generar un modelo configurado con una serie de parámetros dada se aconseja implementar una función get_model, \n",
    "# que recibe tales parámetros como argumentos de entrada, y retorna el modelo correspondiente como salida\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "lrs = [0.0001, 0.001, 0.01] # valores de la learning rate\n",
    "regs = [0.0, 0.0001] # valores de regularización L2\n",
    "drs = [0.0, 0.2, 0.5] # valores de dropout\n",
    "\n",
    "num_epochs = 100\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=1) # para parar el entrenamiento antes de num_epochs atendiendo al error en validación\n",
    "\n",
    "# Imprimimos por pantalla los valores valores de LOSS y ACCURACY de entrenamiento y validación de cada modelo, época a epóca\n",
    "histories = {}\n",
    "for lr in lrs:\n",
    "    for reg in regs:\n",
    "        for dr in drs:\n",
    "            # TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "R7GSpxtL1ATM",
    "outputId": "af1cd892-2b4a-4807-8679-53992570807f"
   },
   "outputs": [],
   "source": [
    "# Obtenemos y visualizamos los parámetros de la mejor época atendiendo a los valores de ACCURACY en validación\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FYRNhAWkMf8"
   },
   "source": [
    "# ***Ejercicio 2 - LSTMs***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdDDxod6ERjW"
   },
   "source": [
    "# Preprocesado de datos - Tokenization y sequence padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "hIlsrhYiEEsR",
    "outputId": "3aadb841-7b15-478c-b53d-89cd3254f3c8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "# Creamos un tokenier que considera las 10000 palabras más frecuentes en el corpus\n",
    "max_words = 10000\n",
    "tokenizer = preprocessing.text.Tokenizer(num_words=max_words)\n",
    "\n",
    "# Construimos el índice (diccionario) de palabras a partir del conjunto de datos de entrenamiento\n",
    "tokenizer.fit_on_texts(training_texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Obtenemos los textos como secuencias de enteros\n",
    "trainining_sequences = tokenizer.texts_to_sequences(training_texts)\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "# Padding: convertimos las secuencias de enteros a un tensor 2D de forma (numero_secuencias, longitud_secuencia)\n",
    "# Con ello igualamos la longitud de las secuencias de entrada\n",
    "max_seq = 40\n",
    "x_training = preprocessing.sequence.pad_sequences(trainining_sequences, maxlen=max_seq)\n",
    "x_validation = preprocessing.sequence.pad_sequences(validation_sequences, maxlen=max_seq)\n",
    "x_test = preprocessing.sequence.pad_sequences(test_sequences, maxlen=max_seq)\n",
    "\n",
    "y_traininig = training_labels\n",
    "y_validation = validation_labels\n",
    "y_test = test_labels\n",
    "\n",
    "print('Dimensiones del conjunto de entrenamiento: {}'.format(x_training.shape))\n",
    "print('Dimensiones del conjunto de validación: {}'.format(x_validation.shape))\n",
    "print('Dimensiones del conjunto de test: {}'.format(x_test.shape))\n",
    "\n",
    "# Visualizamos un par de instancias de entrenamiento\n",
    "print('\\nTexto: {}\\nVector padded: {}'.format(training_texts.iloc[0], x_training[0]))\n",
    "print('\\nTexto: {}\\nVector padded: {}'.format(training_texts.iloc[1], x_training[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cQ9EZbHMpGD"
   },
   "source": [
    "# Construcción de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "0najy18hqR_R",
    "outputId": "1fe66e8c-2bc8-41f6-a508-bc5fe02ad527"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM\n",
    "\n",
    "embedding_size = 128\n",
    "lstm_hidden_size = 128\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Añadimos una capa de embeddings\n",
    "# Usamos mask_zero=True para ignorar los token '0' en el padding\n",
    "# Tras la capa de embeddings, las activaciones tienen la forma (batch_size, max_seq, embedding_size)\n",
    "model.add(Embedding(max_words, embedding_size, mask_zero=True))\n",
    "\n",
    "# Añadimos una capa LSTM\n",
    "model.add(LSTM(lstm_hidden_size))\n",
    "\n",
    "# Añadimos una capa de salida\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ajwZPhpcOwfw",
    "outputId": "c3645551-77c3-4975-f62e-63fdd278d496"
   },
   "outputs": [],
   "source": [
    "# Entrenamos la red durante 20 épocas y con batches de tamaño 128\n",
    "history_lstm = model.fit(x_training, y_training, epochs=20, batch_size=128, validation_data=(x_validation, y_validation), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTKf1gPtp20r"
   },
   "source": [
    "**Ejercicio 2a - LSTM de una capa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 962
    },
    "id": "QmY7ZnIBPxsQ",
    "outputId": "a9c571b2-b831-470c-b165-d7290c5c575a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visulizamos la evolución del LOSS y el ACCURACY de entrenamiento y validación por épocas de entrenamiento\n",
    "# TO DO\n",
    "\n",
    "# Evaluamos el modelo en test\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9rNFcazRrZn"
   },
   "source": [
    "**Ejercicio 2b - Tamaño de embeddings y número de unidades LSTM**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-F8tyO3RR8xa"
   },
   "source": [
    "Prueba diferentes tamaños de embeddings y número de unidades LSTM.\n",
    "¿Observas alguna diferencia en las curvas de pérdida y precisión?\n",
    "¿Qué sucede con un tamaño de embeddings muy pequeño (por ejemplo, 8) o unidades LSTM (por ejemplo, 16)? ¿Y qué ocurre cuando hacemos lo contrario?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osm5pfxdSQGL"
   },
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ0VyMDebA-G"
   },
   "source": [
    "# ***Ejercicio 3 - LSTMs avanzadas***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXDfTXgVTyuS"
   },
   "source": [
    "**Ejercicio 3a - Stacked LSTMs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1149
    },
    "id": "6bZ0eS6IUIro",
    "outputId": "02c38e44-a207-490d-9613-d352c9d3009c"
   },
   "outputs": [],
   "source": [
    "# Creamos un stack con capa de entrada de embeddings, dos capas intermedias (ocultas) de LSTMs, y una capa de salida con función de activación sigmoidal\n",
    "model = Sequential()\n",
    "# TO DO\n",
    "\n",
    "# Usamos el optimizador RMSProp (Root Mean Square Propagation) appropiado para RNNs\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "history_lstm_stacked = model.fit(x_training, y_training, epochs=10, batch_size=128, validation_data=(x_validation, y_validation), verbose=1)\n",
    "\n",
    "plt.plot(history_lstm_stacked.history['loss'])\n",
    "plt.plot(history_lstm_stacked.history['val_loss'])\n",
    "plt.title('Stacked LSTM')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('época')\n",
    "plt.legend(['entrenamiento', 'validación'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Evaluamos el modelo en test\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81SA6CjAaVMY"
   },
   "source": [
    "**Ejercicio 3b - Bidirectional LSTMs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1479
    },
    "id": "J2yD3V56aa0F",
    "outputId": "1bb54020-c24b-4e3f-b011-10b011fa24cc"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "max_words = 10000\n",
    "embedding_size = 50\n",
    "\n",
    "# Creamos una red con una capa de entrada de embeddings, una capa intermedia (oculta) con una biLSTM de tamaño 128, y una capa de salida con función de activación sigmoidal\n",
    "model = Sequential()\n",
    "# TO DO\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Entrenamos el modelo\n",
    "history_bidirectional = model.fit(x_training, y_training, epochs=10, batch_size=128, validation_data=(x_validation, y_validation), verbose=2)\n",
    "\n",
    "plt.plot(history_bidirectional.history['loss'])\n",
    "plt.plot(history_bidirectional.history['val_loss'])\n",
    "plt.title('LSTM bidirectional')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('épocas')\n",
    "plt.legend(['entrenamiento', 'validación'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Evaluamos el modelo en test\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: \", score[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
